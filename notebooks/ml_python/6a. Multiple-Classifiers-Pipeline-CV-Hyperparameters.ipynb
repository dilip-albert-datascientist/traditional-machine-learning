{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in pickled data\n",
    "X_y_data = pd.read_pickle(\"../data/interim/X_y_data.pkl\")\n",
    "X = pd.read_pickle(\"../data/interim/X.pkl\")\n",
    "y = pd.read_pickle(\"../data/interim/y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://scikit-learn.org/stable/_static/ml_map.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which algorithms/estimators are options?\n",
    "    #The estimator you choose for your project will depend on the data set you have and the problem that you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# Read in the data\n",
    "import pandas as pd\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pipeline, Gridsearch, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Plot the confusion matrix and metrics at the end\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "df = pd.read_csv('cell_phones.csv')\n",
    "\n",
    "# Set variables for the targets and features\n",
    "y = df['price_range']\n",
    "X = df.drop('price_range', axis=1)\n",
    "\n",
    "# Split the data into training test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('LR', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Decision Trees pipeline\n",
    "pipe_dt = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('DT',DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Random Forest pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('RF',RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# K-Nearest Neighbors pipeline\n",
    "pipe_knn = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('KNN', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Support Vector Machines pipeline\n",
    "pipe_svm = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('SVM', svm.SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# XGBoost pipeline\n",
    "pipe_xgb = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('XGB', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES WITH PCA\n",
    "\n",
    "# Logistic Regression w/ PCA pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('LR', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Decision Trees w/ PCA pipeline\n",
    "pipe_dt = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('DT',DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Random Forest w/ PCA pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('RF',RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# K-Nearest Neighbors w/ PCA pipeline\n",
    "pipe_knn = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('KNN', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Support Vector Machines w/ PCA pipeline\n",
    "pipe_svm = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('SVM', svm.SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# XGBoost w/ PCA pipeline\n",
    "pipe_xgb = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('XGB', XGBClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER OPTIONS\n",
    "\n",
    "# Logistic Regression hyperparameter options\n",
    "\n",
    "# Logistic regression does not really have any critical hyperparameters to tune.\n",
    "\n",
    "lr_param_grid = [{\n",
    "#     Regularization (penalty) can sometimes be helpful.\n",
    "    'LR__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "#     Note: not all solvers support all regularization terms.\n",
    "#     The C parameter controls the penality strength, which can also be effective.\n",
    "    'LR__C': [100, 10, 1.0, 0.1, 0.01],\n",
    "#     Sometimes, you can see useful differences in performance or convergence with different solvers (solver).\n",
    "    'LR__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "]\n",
    "\n",
    "# Decision Trees hyperparameter options\n",
    "\n",
    "dt_param_grid = [{\n",
    "    'DT__criterion': [],\n",
    "    'DT__min_samples_leaf': [],\n",
    "    'DT__max_depth': [],\n",
    "    'DT__min_samples_split': []}\n",
    "]\n",
    "\n",
    "# Random Forest hyperparameter options\n",
    "\n",
    "The most important parameter is the number of random features to sample at each split point (max_features).\n",
    "\n",
    "rf_param_grid = [{\n",
    "    'RF__min_samples_leaf': [],\n",
    "    'RF__max_depth': [],\n",
    "    'RF__min_samples_split': []}\n",
    "]\n",
    "\n",
    "# K-Nearest Neighbors hyperparameter options\n",
    "\n",
    "# The most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n",
    "\n",
    "knn_param_grid = [{    \n",
    "#     Test values between at least 1 and 21, perhaps just the odd numbers.\n",
    "    'KNN__n_neighbors': [1 to 21],\n",
    "#     It may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\n",
    "    'KNN__weights': ['uniform', 'distance'],\n",
    "#     It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n",
    "    'KNN__metric': ['euclidean', 'manhattan', 'minkowski']}\n",
    "]\n",
    "\n",
    "# Support Vector Machines hyperparameter options\n",
    "\n",
    "# The SVM algorithm, like gradient boosting, is very popular, very effective, \n",
    "# and provides a large number of hyperparameters to tune.\n",
    "\n",
    "svm_param_grid = [{    \n",
    "#     Perhaps the first important parameter is the choice of kernel that will control\n",
    "#     the manner in which the input variables will be projected. \n",
    "#     There are many to choose from, but linear, polynomial, and RBF are the most common, \n",
    "#     perhaps just linear and RBF in practice.\n",
    "    'SVM__kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "#     If the polynomial kernel works out, then it is a good idea to dive into the degree hyperparameter.\n",
    "#     Another critical parameter is the penalty (C) that can take on a range of values \n",
    "#     and has a dramatic effect on the shape of the resulting regions for each class. \n",
    "#     A log scale might be a good starting point.\n",
    "    'SVM__C': [100, 10, 1.0, 0.1, 0.001]}\n",
    "]\n",
    "\n",
    "# XGBoost parameter hyperparameter options\n",
    "xgb_param_grid = [{\n",
    "    'XGB__learning_rate': [],\n",
    "    'XGB__max_depth': [],\n",
    "    'XGB__min_child_weight': [],\n",
    "    'XGB__subsample': [],\n",
    "    'XGB__n_estimators': []}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOSEN HYPERPARAMETER GRIDS\n",
    "\n",
    "param_range = [1, 2, 3, 4, 5, 6]\n",
    "param_range_fl = [1.0, 0.5, 0.1]\n",
    "n_estimators = [50,100,150]\n",
    "learning_rates = [.1,.2,.3]\n",
    "\n",
    "# Logistic Regression hyperparameter grid\n",
    "lr_param_grid = [{\n",
    "    'LR__penalty': ['l1', 'l2'],\n",
    "    'LR__C': param_range_fl,\n",
    "    'LR__solver': ['liblinear']}\n",
    "]\n",
    "\n",
    "# Decision Trees hyperparameter grid\n",
    "dt_param_grid = [{\n",
    "    'DT__criterion': ['gini', 'entropy'],\n",
    "    'DT__min_samples_leaf': param_range,\n",
    "    'DT__max_depth': param_range,\n",
    "    'DT__min_samples_split': param_range[1:]}\n",
    "]\n",
    "\n",
    "# Random Forest hyperparameter grid\n",
    "rf_param_grid = [{\n",
    "    'RF__min_samples_leaf': param_range,\n",
    "    'RF__max_depth': param_range,\n",
    "    'RF__min_samples_split': param_range[1:]}\n",
    "]\n",
    "\n",
    "# K-Nearest Neighbors hyperparameter grid\n",
    "knn_param_grid = [{\n",
    "    'KNN__n_neighbors': param_range,\n",
    "    'KNN__weights': ['uniform', 'distance'],\n",
    "    'KNN__metric': ['euclidean', 'manhattan']}\n",
    "]\n",
    "\n",
    "# Support Vector Machines hyperparameter grid\n",
    "svm_param_grid = [{\n",
    "    'SVM__kernel': ['linear', 'rbf'], \n",
    "    'SVM__C': param_range}\n",
    "]\n",
    "\n",
    "# XGBoost hyperparameter grid\n",
    "xgb_param_grid = [{\n",
    "    'XGB__learning_rate': learning_rates,\n",
    "    'XGB__max_depth': param_range,\n",
    "    'XGB__min_child_weight': param_range[:2],\n",
    "    'XGB__subsample': param_range_fl,\n",
    "    'XGB__n_estimators': n_estimators}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP GRID SEARCH CV\n",
    "\n",
    "scoring = 'accuracy'\n",
    "cv = 3\n",
    "\n",
    "# Logistic Regression grid search CV\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=pipe_lr,\n",
    "    param_grid=lr_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# Decision Trees grid search CV\n",
    "dt_grid_search = GridSearchCV(\n",
    "    estimator=pipe_dt,\n",
    "    param_grid=dt_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# Random Forest grid search CV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=pipe_rf,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# K-Nearest Neighbors grid search CV\n",
    "knn_grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# Support Vector Machines grid search CV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=pipe_svm,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")\n",
    "\n",
    "# XGBoost grid search CV\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=pipe_xgb,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring=scoring,\n",
    "    cv=cv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT MODELS\n",
    "\n",
    "grids = [\n",
    "    lr_grid_search,\n",
    "    dt_grid_search,\n",
    "    rf_grid_search,\n",
    "    knn_grid_search,\n",
    "    svm_grid_search,\n",
    "    xgb_grid_search\n",
    "]\n",
    "\n",
    "for pipe in grids:\n",
    "    pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT & SCORE MODELS\n",
    "\n",
    "grid_dict = {\n",
    "    0: 'Logistic Regression', \n",
    "    1: 'Decision Trees', \n",
    "    2: 'Random Forest', \n",
    "    3: 'K-Nearest Neighbors', \n",
    "    4: 'Support Vector Machines', \n",
    "    5: 'XGBoost'\n",
    "}\n",
    "\n",
    "for i, model in enumerate(grids):\n",
    "    print('{} Test Accuracy: {}'.format(grid_dict[i], model.score(X_test,y_test)))\n",
    "    print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT & SCORE MODELS\n",
    "\n",
    "grid_dict = {\n",
    "    0: 'Logistic Regression', \n",
    "    1: 'Decision Trees', \n",
    "    2: 'Random Forest', \n",
    "    3: 'K-Nearest Neighbors', \n",
    "    4: 'Support Vector Machines', \n",
    "    5: 'XGBoost'\n",
    "}\n",
    "\n",
    "for i, model in enumerate(grids):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print('{} Test Accuracy: {}'.format(grid_dict[i], model.score(X_test,y_test)))\n",
    "    print('{} Best Params: {}'.format(grid_dict[i], model.best_params_))\n",
    "    \n",
    "    print('{} Test Accuracy 2: {}'.format(grid_dict[i], accuracy_score(y_test, y_pred)))\n",
    "    print('{} Test Precision: {}'.format(grid_dict[i], precision_score(y_test, y_pred)))\n",
    "    print('{} Test Recall: {}'.format(grid_dict[i], recall_score(y_test, y_pred)))\n",
    "    print('{} Test F1 Score: {}'.format(grid_dict[i], f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICT & SCORE MODELS\n",
    "\n",
    "grid_dict = {\n",
    "    0: 'Logistic Regression', \n",
    "    1: 'Decision Trees', \n",
    "    2: 'Random Forest', \n",
    "    3: 'K-Nearest Neighbors', \n",
    "    4: 'Support Vector Machines', \n",
    "    5: 'XGBoost'\n",
    "}\n",
    "\n",
    "# Fit the grid search objects\n",
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    \n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    y_pred = gs.predict(X_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n",
    "    # Track best (highest test accuracy) model\n",
    "    if accuracy_score(y_test, y_pred) > best_acc:\n",
    "        best_acc = accuracy_score(y_test, y_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "        \n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n",
    "\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best model based on scoring above\n",
    "clf_best = XXXXX_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(clf_best, X_test, y_test)\n",
    "RocCurveDisplay.from_estimator(clf_best, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_best.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.decision_function(X_test)\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test\n",
    "y_pred = clf_best.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred, normalize=True))\n",
    "print(precision_score(y_true, y_pred, average='binary'))\n",
    "print(recall_score(y_true, y_pred, average='binary'))\n",
    "print(f1_score(y_true, y_pred, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://pandas-ml.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.scikit-yb.org/en/latest/"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
