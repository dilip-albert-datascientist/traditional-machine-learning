{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filepath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(relative_filepath)\n",
    "\n",
    "# for path in sys.path:\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled combined data\n",
    "X_y_data = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_data.pkl\")\n",
    "\n",
    "# Read in pickled train data\n",
    "X_y_train = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_train.pkl\")\n",
    "\n",
    "# Read in pickled test data\n",
    "X_y_test = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_test.pkl\")\n",
    "\n",
    "# Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_ml_missing_data = json.load(open(relative_filepath + \"reports/dicts/dict_ml_missing_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values for config dict\n",
    "input_dfs = [X_y_data,\n",
    "             X_y_train,\n",
    "             X_y_test]\n",
    "\n",
    "target = \"classLabel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Checklist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.justintodata.com/data-cleaning-python-ultimate-guide/\n",
    "\n",
    "Table Of Contents\n",
    "Missing data\n",
    "Irregular data (Outliers)\n",
    "Unnecessary data\n",
    "Unnecessary type #1: Uninformative / Repetitive\n",
    "Unnecessary type #2: Irrelevant\n",
    "Unnecessary type #3: Duplicates\n",
    "Inconsistent data\n",
    "Inconsistent type #1: Capitalization\n",
    "Inconsistent type #2: Formats\n",
    "Inconsistent type #3: Categorical Values\n",
    "Inconsistent type #4: Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://elitedatascience.com/data-cleaning\n",
    "\n",
    "Remove Unwanted observations\n",
    "    Duplicate observations\n",
    "    Irrelevant observations\n",
    "    \n",
    "Fix Structural Errors\n",
    "\n",
    "Filter Unwanted Outliers\n",
    "\n",
    "Handle Missing Data\n",
    "    Missing categorical data\n",
    "    Missing numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# \n",
    "from sklearn import set_config                      # to change the display\n",
    "from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation / Scoring metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y = True, as_frame=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 455 entries, 456 to 257\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              455 non-null    float64\n",
      " 1   mean texture             455 non-null    float64\n",
      " 2   mean perimeter           455 non-null    float64\n",
      " 3   mean area                455 non-null    float64\n",
      " 4   mean smoothness          455 non-null    float64\n",
      " 5   mean compactness         455 non-null    float64\n",
      " 6   mean concavity           455 non-null    float64\n",
      " 7   mean concave points      455 non-null    float64\n",
      " 8   mean symmetry            455 non-null    float64\n",
      " 9   mean fractal dimension   455 non-null    float64\n",
      " 10  radius error             455 non-null    float64\n",
      " 11  texture error            455 non-null    float64\n",
      " 12  perimeter error          455 non-null    float64\n",
      " 13  area error               455 non-null    float64\n",
      " 14  smoothness error         455 non-null    float64\n",
      " 15  compactness error        455 non-null    float64\n",
      " 16  concavity error          455 non-null    float64\n",
      " 17  concave points error     455 non-null    float64\n",
      " 18  symmetry error           455 non-null    float64\n",
      " 19  fractal dimension error  455 non-null    float64\n",
      " 20  worst radius             455 non-null    float64\n",
      " 21  worst texture            455 non-null    float64\n",
      " 22  worst perimeter          455 non-null    float64\n",
      " 23  worst area               455 non-null    float64\n",
      " 24  worst smoothness         455 non-null    float64\n",
      " 25  worst compactness        455 non-null    float64\n",
      " 26  worst concavity          455 non-null    float64\n",
      " 27  worst concave points     455 non-null    float64\n",
      " 28  worst symmetry           455 non-null    float64\n",
      " 29  worst fractal dimension  455 non-null    float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 110.2 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.dtypes\n",
    "X_train.shape\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = X_train.select_dtypes(include=['object']).columns\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# cat_features = []\n",
    "# num_features = []\n",
    "\n",
    "# print(cat_features)\n",
    "# print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_features)\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing.outliers import boxplot_numeric_features, IQR_Outliers, CustomSampler_IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "rows_for_plot = 6\n",
    "cols_for_plot = 5\n",
    "###\n",
    "\n",
    "boxplot_numeric_features(X_train,\n",
    "                         rows_for_plot=rows_for_plot,\n",
    "                         cols_for_plot=cols_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR_Outliers(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomSampler_IQR(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>11.63</td>\n",
       "      <td>29.29</td>\n",
       "      <td>74.87</td>\n",
       "      <td>415.1</td>\n",
       "      <td>0.09357</td>\n",
       "      <td>0.08574</td>\n",
       "      <td>0.07160</td>\n",
       "      <td>0.02017</td>\n",
       "      <td>0.1799</td>\n",
       "      <td>0.06166</td>\n",
       "      <td>...</td>\n",
       "      <td>13.12</td>\n",
       "      <td>38.81</td>\n",
       "      <td>86.04</td>\n",
       "      <td>527.8</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>0.20310</td>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.06835</td>\n",
       "      <td>0.2884</td>\n",
       "      <td>0.07220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>10.66</td>\n",
       "      <td>15.15</td>\n",
       "      <td>67.49</td>\n",
       "      <td>349.6</td>\n",
       "      <td>0.08792</td>\n",
       "      <td>0.04302</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1928</td>\n",
       "      <td>0.05975</td>\n",
       "      <td>...</td>\n",
       "      <td>11.54</td>\n",
       "      <td>19.20</td>\n",
       "      <td>73.20</td>\n",
       "      <td>408.3</td>\n",
       "      <td>0.1076</td>\n",
       "      <td>0.06791</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>0.06164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>15.28</td>\n",
       "      <td>22.41</td>\n",
       "      <td>98.92</td>\n",
       "      <td>710.6</td>\n",
       "      <td>0.09057</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.05375</td>\n",
       "      <td>0.03263</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>0.06317</td>\n",
       "      <td>...</td>\n",
       "      <td>17.80</td>\n",
       "      <td>28.03</td>\n",
       "      <td>113.80</td>\n",
       "      <td>973.1</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>0.32990</td>\n",
       "      <td>0.36300</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.09772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16.02</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.19</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>12.72</td>\n",
       "      <td>17.67</td>\n",
       "      <td>80.98</td>\n",
       "      <td>501.3</td>\n",
       "      <td>0.07896</td>\n",
       "      <td>0.04522</td>\n",
       "      <td>0.01402</td>\n",
       "      <td>0.01835</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.05544</td>\n",
       "      <td>...</td>\n",
       "      <td>13.82</td>\n",
       "      <td>20.96</td>\n",
       "      <td>88.87</td>\n",
       "      <td>586.8</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.09605</td>\n",
       "      <td>0.03469</td>\n",
       "      <td>0.03612</td>\n",
       "      <td>0.2165</td>\n",
       "      <td>0.06025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "456        11.63         29.29           74.87      415.1          0.09357   \n",
       "174        10.66         15.15           67.49      349.6          0.08792   \n",
       "184        15.28         22.41           98.92      710.6          0.09057   \n",
       "10         16.02         23.24          102.70      797.8          0.08206   \n",
       "429        12.72         17.67           80.98      501.3          0.07896   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "456           0.08574         0.07160              0.02017         0.1799   \n",
       "174           0.04302         0.00000              0.00000         0.1928   \n",
       "184           0.10520         0.05375              0.03263         0.1727   \n",
       "10            0.06669         0.03299              0.03323         0.1528   \n",
       "429           0.04522         0.01402              0.01835         0.1459   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "456                 0.06166  ...         13.12          38.81   \n",
       "174                 0.05975  ...         11.54          19.20   \n",
       "184                 0.06317  ...         17.80          28.03   \n",
       "10                  0.05697  ...         19.19          33.88   \n",
       "429                 0.05544  ...         13.82          20.96   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "456            86.04       527.8            0.1406            0.20310   \n",
       "174            73.20       408.3            0.1076            0.06791   \n",
       "184           113.80       973.1            0.1301            0.32990   \n",
       "10            123.80      1150.0            0.1181            0.15510   \n",
       "429            88.87       586.8            0.1068            0.09605   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "456          0.29230               0.06835          0.2884   \n",
       "174          0.00000               0.00000          0.2710   \n",
       "184          0.36300               0.12260          0.3175   \n",
       "10           0.14590               0.09975          0.2948   \n",
       "429          0.03469               0.03612          0.2165   \n",
       "\n",
       "     worst fractal dimension  \n",
       "456                  0.07220  \n",
       "174                  0.06164  \n",
       "184                  0.09772  \n",
       "10                   0.08452  \n",
       "429                  0.06025  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(455, 30) (455,)\n",
      "Accuracy on Train set: 0.989010989010989 \n",
      "\n",
      "Accuracy on Test set: 0.956140350877193 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(433, 30) (433,)\n",
      "Accuracy on Train set: 0.9907621247113164 \n",
      "\n",
      "Accuracy on Test set: 0.9473684210526315 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# # select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
    "\n",
    "# # evaluate the model\n",
    "# yhat = model.predict(X_test)\n",
    "\n",
    "# # evaluate predictions\n",
    "# mae = mean_absolute_error(y_test, yhat)\n",
    "# print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "# Do i want to remove outliers from test dataset? Without the outlier tows then no prediction can be made\n",
    "LR_Pipeline = Pipeline([\n",
    "    ('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False)),\n",
    "    ('Imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat_transformer', cat_pipe, cat_features),\n",
    "    ('num_transformer', num_pipe, num_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict training data\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "# print(f\"Predictions on training data: {y_train_pred}\")\n",
    "print(\"Accuracy on Training set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "# print(f\"Predictions on test data: {y_test_pred}\")\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config to diagram for visualizing the pipelines/composite estimators\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Lets visualize the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Num_vars is the list of numerical variables \n",
    "X_train_num = X_train[numeric_features]\n",
    "X_train_num = imputer.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_train_cat_ord_encoded = ordinal_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_ord_encoded[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "X_train_cat_hot_encoded = cat_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome transformations\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ratings_index = -2\n",
    "reviews_index = -1\n",
    "class NewVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "    # Make a new variable that is rating divided by number of reviews\n",
    "        ratings_over_reviews = X[:,ratings_index]/X[:,reviews_index]\n",
    "        return np.c_[X, ratings_over_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('add_variables', NewVariablesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_num_transformed = num_pipeline.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, num_vars),\n",
    "    ('categorical', OneHotEncoder(), cat_vars),\n",
    "    \n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('features', FeatureUnion ([\n",
    "     ('Cat Columns', Pipeline([\n",
    "          ('Category Extractor', TypeSelector(np.number)),\n",
    "                 ('Impute Zero', SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "                                    ])),\n",
    "('Numerics', Pipeline([\n",
    "      ('Numeric Extractor', TypeSelector(\"category\")),\n",
    "          ('Impute Missing', SimpleImputer(strategy=\"constant\", fill_value='missing'))\n",
    "          ]))        \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median', fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "print (\"## Number of duplicate rows ## \\n\")\n",
    "if len(X_y_data[X_y_data.duplicated()]) > 0:\n",
    "    print(\"Number of duplicated observations: \", len(X_y_data[X_y_data.duplicated()]))\n",
    "    X_y_data[X_y_data.duplicated(keep=False)].sort_values(by=list(X_y_data.columns)).head()\n",
    "else:\n",
    "    print(\"No duplicated observations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_y_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with a lot of missing values.\n",
    "ind_missing = df[df['num_missing'] > 35].index\n",
    "df_less_missing_rows = df.drop(ind_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_beds_raion has a lot of missing.\n",
    "# If we want to drop.\n",
    "cols_to_drop = ['hospital_beds_raion']\n",
    "df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with the median.\n",
    "med = df['life_sq'].median()\n",
    "print(med)\n",
    "df['life_sq'] = df['life_sq'].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each numeric column.\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        med = df[col].median()\n",
    "        df[col] = df[col].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each non-numeric column.\n",
    "df_non_numeric = df.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = df[col].describe()['top'] # impute with the most frequent value.\n",
    "        df[col] = df[col].fillna(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "df['sub_area'] = df['sub_area'].fillna('_MISSING_')\n",
    "\n",
    "\n",
    "# numeric\n",
    "df['life_sq'] = df['life_sq'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This article covers 7 ways to handle missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deleting Rows with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for continuous variable\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].mean())\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other Imputation Methods\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].fillna(method='ffill')\n",
    "data[\"Age\"] = data[\"Age\"].interpolate(method='linear', limit_direction='forward', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Algorithms that support missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction of missing values\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]]\n",
    "\n",
    "data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]\n",
    "\n",
    "test_data = data[data[\"Age\"].isnull()]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "y_train = data[\"Age\"]\n",
    "X_train = data.drop(\"Age\", axis=1)\n",
    "X_test = test_data.drop(\"Age\", axis=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation using Deep Learning Library — Datawig\n",
    "\n",
    "import pandas as pd\n",
    "#pip install datawig\n",
    "import datawig\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(data)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['Pclass','SibSp','Parch'], # column(s) containing information about the column we want to impute\n",
    "    output_column= 'Age', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "imputed = imputer.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do All Data Cleaning on Only Train Set and Apply Calculations to Validation/Test Later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_ml_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://miro.medium.com/max/700/1*_RA3mCS30Pr0vUxbp25Yxw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Category Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_category_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Rescaling: Standardise/Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Other Distribution Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO LOOK INTO NEXT PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ENCODER LOOK INTO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS FOR ALL PREPROCESSING STEPS TO FEED INTO PIPELINES\n",
    "\n",
    "imputers\n",
    "scaler\n",
    "pca\n",
    "smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
