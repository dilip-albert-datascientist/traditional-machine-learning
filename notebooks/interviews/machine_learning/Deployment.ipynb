{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "All these algos are micro-services written in Python (please refer to the Python Specifics section below for more info) that reads consume sales data from the Postgres database.\n",
    "\n",
    "Each algo is deployed as an HTTP service that receives requests, reads the necessary data from the database, performs the calculations, and returns a JSON response to the client. The requests typically use the same arguments:\n",
    "\n",
    "Item ID, the integer ID of the item in the database, e.g. 1842589.\n",
    "Item group code, a string identifying a rough category of items, e.g. 'PTV_FLAT'.\n",
    "Country code, e.g. 'DE'.\n",
    "Period sequence, an integer identifying a particular week number since an epoch, e.g. 2670 (our algos only support 'Weekly' periodicity currently).\n",
    "\n",
    "Newron is deployed to GCP using Kubernetes, upon merging to master the CI will produce a Docker image tagged with the commit hash and stored in the container registry. Upon release the release manager will choose the commit hash corresponding to the image which will be deployed to the target environment."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ecosystem Services and Dataflow\n",
    "\n",
    "Currently each environment has two \"instances\", akin to sub-environments, dataprep and ecosystem.\n",
    "\n",
    "Dataprep\n",
    "\n",
    "This is the main instance for algos as it is where pre-calc runs, pre-calc results are saved to the dataload DB. There's a backup and restore process in place that copies the data in this DB to the api DB in the ecosystem instance.\n",
    "\n",
    "Ecosystem\n",
    "\n",
    "This is where the public UI and API are available. They consume data from the api DB, which is a copy of the dataload one in Dataprep.\n",
    "\n",
    "Algos are also available here for on-demand requests, usually consuming different endpoints than the ones pre-calc uses. A couple use cases are:\n",
    "\n",
    "TPRC for running promotion planning simulations.\n",
    "TPR/M for getting aggregated TPR KPIs for user-defined time periods, this is to make sure the data we display meets confidentiality rules."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Python specifics\n",
    "All algos are written in Python, using Pandas for calculations, psycopg2 for interaction with the database, Flask as the web framework, and Gunicorn as a WSGI server.\n",
    "\n",
    "Generally they follow the same pattern:\n",
    "\n",
    "wsgi.py is the entry point, creating the Flask application for Gunicorn to use.\n",
    "gunicorn[.<environment>].py contains gunicorn' config, used when running the application in production mode.\n",
    "precalc.py. is the entry point being introduced for all algos to run as a cli, used by CbC/Argo.\n",
    "All algos have a main module, usually named after the algo's abbreviated name (pp, pe, tprc, moc). The flask app setup can be found at the top level of this module as well as the logic for alternative entry point like precalc_flow.py for CbC.\n",
    "\n",
    "main.py includes the application factory setting up the routes, exception handlers, etc.\n",
    "endpoints.py defines the request handler functions for the flask app.\n",
    "config.py holds the definition of the configuration, values are read from environment variables.\n",
    "The actual Data Science logic for the algo can then be found in the algo submodule:\n",
    "\n",
    "algo/__init__.py is the entry point to the calculation functions.\n",
    "algo/data_retrieval.py holds the different functions to interact with the database.\n",
    "algo/queries.py holds the SQL queries used for data fetching.\n",
    "algo/data_save.py usually holds the SQL used for persisting data to the DB.\n",
    "Usually there are several modules within algo for the model code, preprocessing, and postprocessing.\n",
    "Please refer to each algo's README for more details."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
