{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step guide to explaining your ML project during a data science interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Selecting a project."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goes without saying, while picking a project to demonstrate your technical prowess, make sure it resonates well with the company you are applying for.\n",
    "\n",
    "For instance, for an e-commerce company, I would go with a retail dataset, for a fintech company I would choose a loan application dataset, and for a healthcare company I would prefer to pick Covid-19 or a breast cancer dataset. The trick is to pick a project based on your target audience. I swear by Kaggle to provide good quality datasets along with some analysis notebooks to get you started.\n",
    "Also, it is actually a good idea to have some end-to-end projects from different sectors under your kitty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explaining the data source."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Begin your explanation by specifying where you got the data to work with.\n",
    "\n",
    "It could be that the data was provided/collected by you at your last company. Maybe you did the project for fun and extracted the required data via Kaggle. You can even mention that it was some open-source data available on the net freely. Perhaps you mined the data (of course ethically) using third party APIs (happens a lot for Twitter data). Whatever be the case, make sure you are revealing the source of your data. Additionally, give a brief overview of some of the columns/features in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explain your objective behind this project."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Specify what is it that you were trying to achieve with this project.\n",
    "\n",
    "It could be a classification problem to separate approved vs. rejected loan applications, a regression problem to predict house prices, a cold-start problem for recommender system, clustering problem to find similar users for targeted advertising. Absolute clarity in terms of explaining what the project was about is paramount from an interviewer’s perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preparing your dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is where you talk about data cleaning, data wrangling, handling outliers, multicollinearity, duplicate removal, feature engineering, feature normalization, etc AND also the techniques to handle each of them.\n",
    "\n",
    "The idea is to mention the actual techniques you used to target each of these data preparation steps mentioned above. For instance, explicitly state that observations with a Cook’s distance of more than 3 times the mean were considered outliers. Values for VIF (Variance Inflation Factor) exceeding 10 were regarded as indicating multicollinearity. One hot encoding (or label encoding) was used to handle categorical data. Numerical data was scaled/normalized to ensure all features are on the same scale. An 80:20 train test split was done to ensure there is no data leakage. Usage of the t-SNE plot to see a visible separation between classes (in case of classification problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: State the KPIs or Performance Metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is it that was most important to your research problem — accuracy, precision, recall, false positives, false negatives, etc?\n",
    "\n",
    "Every ML model has some metrics that you are trying to optimize. It can vary from person-to-person, problem-to-problem, stakeholder-to-stakeholder, and even sector-to-sector. A healthcare data scientist will have to ensure his (or her) model has fewer false negatives as it could cost a patient his life if an incorrect cancer diagnosis is performed. On the other hand, a system installed in a mall to detect shoplifters has to worry about too many false positives as it would mean causing a huge deal of embarrassment to otherwise innocent shoppers.\n",
    "Also, in case of classification problem, make sure you specify whether your dataset was imbalanced or balanced, mainly because your choice of the performance metric is largely dependent on the distribution of classes in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Baseline model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How would you know your created model is any better (than the ones already exisiting out there)?\n",
    "\n",
    "This is one step that is often overlooked but is immensely important. I have seen so many people fumble when the interviewer's follow up question is — So tell me why do you think your model is any good?\n",
    "\n",
    "It is important to have a baseline that you can compare your final model against. More often, you can pick a baseline through a quick google search — whats the highest accuracy achieved on the MNIST dataset? OR whats the accuracy for Netflix’s recommendation system Cinematch. If you cannot find a baseline for your field/problem, you can always create one yourself.\n",
    "\n",
    "A baseline model is one that is simple to set up and has reasonable chances of providing decent results. For instance, in the case of time series forecasting of ice-cream sales, my baseline (read: stupid) model may make predictions for tomorrow solely using data from today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Explain the training process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Explain why a particular algorithm was selected.\n",
    "\n",
    "Always begin with the basic spot-checking several algorithms using cross-validations, followed by selecting the one with the highest value of performance metric (specified in Step 4). In my experience, it always comes down to two or three algorithms which vary only slightly in their performance. Of those three, I personally tend to go with the ensemble model like Random Forest, XG Boost, CART, etc. (especially since they tend to increase prediction accuracy by combining the predictions from multiple models together).\n",
    "But hey, that's my reasoning! You might be comfortable selecting linear regression as your go-to model, especially since it is so easy to explain to non-technical stakeholders. All in all, do remember to share your model-selection decisions with your interviewer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Explain the model tuning process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How did you increase the accuracy of your model? What challenges did you face during this process?\n",
    "\n",
    "The trick is to talk about how you improved accuracy whilst preventing high variance (or overfitting) problem. Talk about hyperparameter tuning using the grid search or randomized search. Also, specify if you used some sort of oversampling/undersampling technique to balance your dataset? Emphasize the use of pipelines during your ML workflow to avoid data leakage and subsequent overfitting. Explain how you used the learning curves to keep track of the loss function over time.\n",
    "As for the challenges, these are unique to one’s personal experience. I will explain my biggest challenge in the sample script at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Model deployment process"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Is your model sitting in your Jupyter Notebook or out in the world for everyone else to enjoy as well?\n",
    "\n",
    "Quite often, interviewers are looking for potential data science recruits who know how to wrap up their model in a nice little container and present it to the world. This could be in the form of a web app or an API. I would highly recommend going that extra mile and learning how to do one of them. Complete beginners can check out this article on how to deploy models as APIs using the Flask framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Prepare some backup questions!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If you were to do something differently, what would that be? If I were to change condition X in this project, how would your approach change?\n",
    "\n",
    "Questions like the one above are bound to be asked once you finish explaining your project. And most of the time, it is a good sign! It means the interviewer was listening and is genuinely interested to know more about your work. Try to think of a few things you would want to differently for your project — for instance, I would try to get access to unbiased data (for instance, one that has equal representation of males & females), I would experiment with stacked models, I would re-assess my confusion matrix with different classification thresholds, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Remember to breathe!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Don’t rush to finish! Enjoy the process and weave a beautiful story for your audience.\n",
    "\n",
    "I know how overwhelming it can get to condense a project that took you 3–4 weeks into a 90-second elevator pitch. But it is not about how much work you did, it’s about how much of it can you convey effectively to the interviewer. So breathe….\n",
    "\n",
    "And now to tie it all up with a sample script!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Sample answer script to this question"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q — So, can you tell us about any projects you did recently?\n",
    "\n",
    "A- For sure. I did quite a few, especially given the lockdown, but I think will pick my favorite, simply because it cleared a lot of data science basics for me. During my Ph.D. internship, I was providing consultations to a company that specialized in giving loans for second-hand cars. They were trying to automate the process of assessing incoming loan applications in the shortest time possible. So basically, I was dealing with a binary classification but with an imbalanced class problem — the number of approved applications was far more compared to rejected applications.\n",
    "\n",
    "Since the data was imbalanced, I decided to use F1 as the performance metric as it reduces both false positives and negatives. The dataset was made available by the company itself and contained features such as age, car type, loan amount, deposit, credit score, etc. As part of the exploratory data analysis phase, I took care of outliers using Cook’s distance, multicollinearity using VIFs, duplicate removal, imputed missing values using KNNImputer, and performed 80:20 split on data. Categorical and numerical features were one hot encoded and normalized, respectively. All the analyses were performing using SkLearn.\n",
    "\n",
    "For the baseline model, I chose a simple model that predicts the application outcome only using the applicant’s credit score. With that, I was able to achieve an F1 score of 56%. For the model selection, I began with spot-checking a few algorithms like SVM, LR, KNN, NN, and RF gave the best cross-validation scores among them. I proceeded with the hyperparameter tuning using GridSearch CV and was able to achieve an F1 score equal to 74% on the test set. I also tried to oversample the rare classes using SMOTEENN library but that didn’t help much in improving performance. What did help was doing some feature reduction using the variance importance plot from RF and finally my F1 score was around 82%.\n",
    "\n",
    "I was satisfied with this and decided to test my model on previously unseen held-out test sets given by the company. I was surprised to find it did not perform as well as I expected. I then referred to a few blog articles to realize what I was doing wrong. Apparently, one-hot encoding categorical features is bad for tree-based models, especially because we create many binary sparse features and from the splitting algorithm’s point of view, they’re all independent. As a result, continuous variables are automatically given higher importance and chosen at the top of the tree to make a split. To alleviate this problem, I switched from SkLearn to H2o framework that does not require categorical features to be one-hot encoded. While my F1 score remained roughly the same, my H2o model generalized better on the unseen dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyperparameter tuning\n",
    "Cross-validation\n",
    "ML workflow pipelines\n",
    "Bias-variance tradeoff\n",
    "Over/Under Sampling\n",
    "Ensemble models\n",
    "Scaling, normalization, one hot encoding\n",
    "Feature reduction, feature engineering\n",
    "Performance metric, confusion matrix"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
