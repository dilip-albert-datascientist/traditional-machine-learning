{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filepath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(relative_filepath)\n",
    "\n",
    "# for path in sys.path:\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled combined data\n",
    "X_y_data = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_data.pkl\")\n",
    "\n",
    "# Read in pickled train data\n",
    "X_y_train = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_train.pkl\")\n",
    "\n",
    "# Read in pickled test data\n",
    "X_y_test = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_test.pkl\")\n",
    "\n",
    "# Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_ml_missing_data = json.load(open(relative_filepath + \"reports/dicts/dict_ml_missing_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values for config dict\n",
    "input_dfs = [X_y_data,\n",
    "             X_y_train,\n",
    "             X_y_test]\n",
    "\n",
    "target = \"classLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Checklist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.justintodata.com/data-cleaning-python-ultimate-guide/\n",
    "\n",
    "Table Of Contents\n",
    "Missing data\n",
    "Irregular data (Outliers)\n",
    "Unnecessary data\n",
    "Unnecessary type #1: Uninformative / Repetitive\n",
    "Unnecessary type #2: Irrelevant\n",
    "Unnecessary type #3: Duplicates\n",
    "Inconsistent data\n",
    "Inconsistent type #1: Capitalization\n",
    "Inconsistent type #2: Formats\n",
    "Inconsistent type #3: Categorical Values\n",
    "Inconsistent type #4: Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://elitedatascience.com/data-cleaning\n",
    "\n",
    "Remove Unwanted observations\n",
    "    Duplicate observations\n",
    "    Irrelevant observations\n",
    "    \n",
    "Fix Structural Errors\n",
    "\n",
    "Filter Unwanted Outliers\n",
    "\n",
    "Handle Missing Data\n",
    "    Missing categorical data\n",
    "    Missing numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalised preprocessing handlers\n",
    "\n",
    "# Numeric  handlers\n",
    "def num_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "def outlier_handler(X):\n",
    "    pass\n",
    "\n",
    "# Categorical handlers\n",
    "def cat_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def label_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def ordinal_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def target_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass\n",
    "\n",
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass\n",
    "\n",
    "def imbalance_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine categorical and numerical features\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL WITH RELEVANT ##\n",
    "\n",
    "# Column dtypes selector\n",
    "numerical_cols = []\n",
    "# imputation_cols = []\n",
    "# power_transform_cols = []\n",
    "# outlier_cols = []\n",
    "# scaling_cols = []\n",
    "categorical_cols = []\n",
    "text_cols = []\n",
    "\n",
    "# Function transformers for numeric pipeline\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[numerical_cols], validate=False)\n",
    "apply_num_imputations = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_power_transforms = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_outlier_handling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_scaling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_balancing = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for categorical pipeline\n",
    "get_categorical_data = FunctionTransformer(lambda x: x[categorical_cols], validate=False)\n",
    "apply_cat_imputations = FunctionTransformer(FUNCTION, validate=False) #SimpleImputer(strategy='most_frequent', fill_value='categorical', missing_values=np.nan)\n",
    "apply_label_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_one_hot_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_ordinal_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for text pipeline\n",
    "get_text_data = FunctionTransformer(lambda x: x[text_cols], validate=False)\n",
    "apply_vectorizer = FunctionTransformer(FUNCTION, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual dtype pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('selector', get_numeric_data),\n",
    "    ('imputer', apply_imputations),\n",
    "    ('power_transformer', apply_power_transforms),\n",
    "    ('outliers', apply_outlier_handling)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('selector', get_categorical_data),\n",
    "    ('imputer', apply_cat_imputations),\n",
    "    ('le', apply_label_encoding),\n",
    "    ('ohe', apply_one_hot_encoding),\n",
    "    ('ordinal', apply_ordinal_encoding)\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('selector', get_text_data),\n",
    "    ('vectorizer', apply_vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with feature union\n",
    "preprocessor_pl = FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_transformer),\n",
    "        ('categorical', categorical_transformer),\n",
    "        ('text', text_transformer)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "preprocessor_pl = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_pipeline),\n",
    "        ('categorical', categorical_pipeline),\n",
    "        ('text', text_pipeline)\n",
    "    ])),\n",
    "    \n",
    "#     ('scaler', apply_scaling),\n",
    "#     ('imbalance', apply_balancing),\n",
    "#     ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with column transformer\n",
    "preprocessor_pl = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', preprocessor_pl),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')   \n",
    "\n",
    "# Defining an example pipeline\n",
    "model = Pipeline([('transformer', FunctionTransformer(lambda x: 2*x)), ('clf', LogisticRegression())])\n",
    "\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://assets.datacamp.com/production/repositories/4983/datasets/238dde66d8af1b7ebd8ffe82de9df60ad6a68d22/preprocessing3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery for Building Preprocessing Handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_imputation_handler(X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "def CODE\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "y,fitted_lambda= boxcox(y,lmbda=None)\n",
    "\n",
    "In the sklearn:\n",
    "    \n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "data = pt.fit_transform(data)\n",
    "\n",
    "In SciPy:\n",
    "    \n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "y,fitted_lambda = yeojohnson(y,lmbda=None)\n",
    "\n",
    "In Sklearn:\n",
    "    \n",
    "We can apply the transform by defining a PowerTransform object and setting the “method” argument to “yeo-johnson”\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "data = pt.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "→ The ladder of powers\n",
    "Data transformations are commonly power transformations, x’=xθ (where x’ is the transformed x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/656/1*8jUUiaF9dD9ZiLzH8e_9jA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/872/1*Jwpotn5OKYfkzoGQFYKunA.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the data are right-skewed (clustered at lower values) move down the ladder of powers (that is, try square root, cube root, logarithmic, etc. transformations).\n",
    "\n",
    "If the data are left-skewed (clustered at higher values) move up the ladder of powers (cube, square, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1400/1*RRZ4lakWAhBWRMC9r1r0Ew.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The boxcox() SciPy function implements the Box-Cox method. It takes an argument, called lambda, that controls the type of transform to perform.\n",
    "lambda = -1. is a reciprocal transform.\n",
    "lambda = -0.5 is a reciprocal square root transform.\n",
    "lambda = 0.0 is a log transform.\n",
    "lambda = 0.5 is a square root transform.\n",
    "lambda = 1.0 is no transform.\n",
    "\n",
    "A limitation of the Box-Cox transform is that it assumes that all values in the data sample are positive.\n",
    "Yeo-Johnson Transformation Method\n",
    "Unlike the Box-Cox transform, it does not require the values for each input variable to be strictly positive.\n",
    "It supports zero values and negative values. This means we can apply it to our dataset without scaling it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handler(X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_imputation_handler(X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# label encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# initialise data of lists.\n",
    "dict_X_train = {\n",
    "    'num_col_1': [1, 2, 3, 4],\n",
    "    'cat_col_1': ['Tom', 'nick', 'krish', 'jack'],\n",
    "    'cat_col_2': ['A', 'B', 'C', 'C']\n",
    "}\n",
    "\n",
    "dict_X_test = {\n",
    "    'num_col_1': [1, 2, 3],\n",
    "    'cat_col_1': ['krish', 'jack', 'krish'],\n",
    "    'cat_col_2': ['A', 'B', 'B']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "X_train = pd.DataFrame(dict_X_train)\n",
    "X_test = pd.DataFrame(dict_X_test)\n",
    " \n",
    "# Print the output.\n",
    "X_train.head()\n",
    "# X_test.head()\n",
    "\n",
    "# X_train_ohe = X_train\n",
    "\n",
    "categorical_columns = ['cat_col_1', 'cat_col_2']\n",
    "\n",
    "# for col in categorical_columns:\n",
    "#     col_ohe = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "#     X_train_ohe = pd.concat((X_train_ohe, col_ohe), axis=1).drop(col, axis=1)\n",
    "    \n",
    "# X_train.head()\n",
    "# X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_col_1</th>\n",
       "      <th>cat_col_1_jack</th>\n",
       "      <th>cat_col_1_krish</th>\n",
       "      <th>cat_col_1_nick</th>\n",
       "      <th>cat_col_2_B</th>\n",
       "      <th>cat_col_2_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_col_1  cat_col_1_jack  cat_col_1_krish  cat_col_1_nick  cat_col_2_B  \\\n",
       "0          1             0.0              0.0             0.0          0.0   \n",
       "1          2             0.0              0.0             1.0          1.0   \n",
       "2          3             0.0              1.0             0.0          0.0   \n",
       "3          4             1.0              0.0             0.0          0.0   \n",
       "\n",
       "   cat_col_2_C  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          1.0  \n",
       "3          1.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class My_encoder(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def __init__(self, drop ='first', sparse=False):\n",
    "        self.encoder = OneHotEncoder(drop=drop, sparse=sparse)\n",
    "        self.features_to_encode = []\n",
    "        self.columns = []\n",
    "    \n",
    "    def fit(self, X_train, features_to_encode):\n",
    "        \n",
    "        data = X_train.copy()\n",
    "        self.features_to_encode = features_to_encode\n",
    "        data_to_encode = data[self.features_to_encode]\n",
    "        self.columns = pd.get_dummies(data_to_encode, drop_first=True).columns\n",
    "        self.encoder.fit(data_to_encode)\n",
    "        return self.encoder\n",
    "    \n",
    "    def transform(self, X_test):\n",
    "        \n",
    "        data = X_test.copy()\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "        data_to_encode = data[self.features_to_encode]\n",
    "        data_left = data.drop(self.features_to_encode, axis = 1)\n",
    "        \n",
    "        data_encoded = pd.DataFrame(self.encoder.transform(data_to_encode), columns=self.columns)\n",
    "        \n",
    "        return pd.concat([data_left, data_encoded],axis = 1)\n",
    "\n",
    "# categorical_columns = [---list of features to one hot encode--]\n",
    "enc = My_encoder()\n",
    "enc.fit(X_train, categorical_columns)\n",
    "\n",
    "X_train_ohe = enc.transform(X_train)\n",
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_col_1</th>\n",
       "      <th>cat_col_1_jack</th>\n",
       "      <th>cat_col_1_krish</th>\n",
       "      <th>cat_col_1_nick</th>\n",
       "      <th>cat_col_2_B</th>\n",
       "      <th>cat_col_2_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_col_1  cat_col_1_jack  cat_col_1_krish  cat_col_1_nick  cat_col_2_B  \\\n",
       "0          1             0.0              1.0             0.0          0.0   \n",
       "1          2             1.0              0.0             0.0          1.0   \n",
       "2          3             0.0              1.0             0.0          1.0   \n",
       "\n",
       "   cat_col_2_C  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_ohe = enc.transform(X_test)\n",
    "X_test_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9f0dc47b6eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "def ordinal_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "train_df=pd.DataFrame({'Degree':['High school','Masters','Diploma','Bachelors','Bachelors','Masters','Phd','High school','High school']})\n",
    "\n",
    "# create object of Ordinalencoding\n",
    "encoder= ce.OrdinalEncoder(cols=['Degree'],return_df=True,\n",
    "                           mapping=[{'col':'Degree',\n",
    "'mapping':{'None':0,'High school':1,'Diploma':2,'Bachelors':3,'Masters':4,'phd':5}}])\n",
    "\n",
    "#Original data\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model input transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "###################\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# \n",
    "from sklearn import set_config                      # to change the display\n",
    "from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation / Scoring metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y = True, as_frame=True)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "X.drop('CHAS', axis=1, inplace=True)\n",
    "y = pd.Series(boston.target, name='MEDV')\n",
    "\n",
    "# inspect data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "X_train.shape\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = X_train.select_dtypes(include=['object']).columns\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# cat_features = []\n",
    "# num_features = []\n",
    "\n",
    "# print(cat_features)\n",
    "# print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_features)\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing.outliers import boxplot_numeric_features, IQR_Outliers, CustomSampler_IQR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numeric Outlier (IQR)\n",
    "Z-Score\n",
    "DBSCAN\n",
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "rows_for_plot = 6\n",
    "cols_for_plot = 5\n",
    "###\n",
    "\n",
    "boxplot_numeric_features(X_train,\n",
    "                         rows_for_plot=rows_for_plot,\n",
    "                         cols_for_plot=cols_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before IQR outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after IQR outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before Z-score outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after Z-score outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR_Outliers(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomSampler_IQR(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# # select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
    "\n",
    "# # evaluate the model\n",
    "# yhat = model.predict(X_test)\n",
    "\n",
    "# # evaluate predictions\n",
    "# mae = mean_absolute_error(y_test, yhat)\n",
    "# print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "# Do i want to remove outliers from test dataset? Without the outlier tows then no prediction can be made\n",
    "LR_Pipeline = Pipeline([\n",
    "    ('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False)),\n",
    "    ('Imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat_transformer', cat_pipe, cat_features),\n",
    "    ('num_transformer', num_pipe, num_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict training data\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "# print(f\"Predictions on training data: {y_train_pred}\")\n",
    "print(\"Accuracy on Training set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "# print(f\"Predictions on test data: {y_test_pred}\")\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config to diagram for visualizing the pipelines/composite estimators\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Lets visualize the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Num_vars is the list of numerical variables \n",
    "X_train_num = X_train[numeric_features]\n",
    "X_train_num = imputer.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_train_cat_ord_encoded = ordinal_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_ord_encoded[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "X_train_cat_hot_encoded = cat_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome transformations\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ratings_index = -2\n",
    "reviews_index = -1\n",
    "class NewVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "    # Make a new variable that is rating divided by number of reviews\n",
    "        ratings_over_reviews = X[:,ratings_index]/X[:,reviews_index]\n",
    "        return np.c_[X, ratings_over_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('add_variables', NewVariablesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_num_transformed = num_pipeline.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, num_vars),\n",
    "    ('categorical', OneHotEncoder(), cat_vars),\n",
    "    \n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('features', FeatureUnion ([\n",
    "     ('Cat Columns', Pipeline([\n",
    "          ('Category Extractor', TypeSelector(np.number)),\n",
    "                 ('Impute Zero', SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "                                    ])),\n",
    "('Numerics', Pipeline([\n",
    "      ('Numeric Extractor', TypeSelector(\"category\")),\n",
    "          ('Impute Missing', SimpleImputer(strategy=\"constant\", fill_value='missing'))\n",
    "          ]))        \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median', fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "print (\"## Number of duplicate rows ## \\n\")\n",
    "if len(X_y_data[X_y_data.duplicated()]) > 0:\n",
    "    print(\"Number of duplicated observations: \", len(X_y_data[X_y_data.duplicated()]))\n",
    "    X_y_data[X_y_data.duplicated(keep=False)].sort_values(by=list(X_y_data.columns)).head()\n",
    "else:\n",
    "    print(\"No duplicated observations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_y_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with a lot of missing values.\n",
    "ind_missing = df[df['num_missing'] > 35].index\n",
    "df_less_missing_rows = df.drop(ind_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_beds_raion has a lot of missing.\n",
    "# If we want to drop.\n",
    "cols_to_drop = ['hospital_beds_raion']\n",
    "df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with the median.\n",
    "med = df['life_sq'].median()\n",
    "print(med)\n",
    "df['life_sq'] = df['life_sq'].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each numeric column.\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        med = df[col].median()\n",
    "        df[col] = df[col].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each non-numeric column.\n",
    "df_non_numeric = df.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = df[col].describe()['top'] # impute with the most frequent value.\n",
    "        df[col] = df[col].fillna(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "df['sub_area'] = df['sub_area'].fillna('_MISSING_')\n",
    "\n",
    "\n",
    "# numeric\n",
    "df['life_sq'] = df['life_sq'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This article covers 7 ways to handle missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deleting Rows with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for continuous variable\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].mean())\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other Imputation Methods\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].fillna(method='ffill')\n",
    "data[\"Age\"] = data[\"Age\"].interpolate(method='linear', limit_direction='forward', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Algorithms that support missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction of missing values\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]]\n",
    "\n",
    "data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]\n",
    "\n",
    "test_data = data[data[\"Age\"].isnull()]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "y_train = data[\"Age\"]\n",
    "X_train = data.drop(\"Age\", axis=1)\n",
    "X_test = test_data.drop(\"Age\", axis=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation using Deep Learning Library — Datawig\n",
    "\n",
    "import pandas as pd\n",
    "#pip install datawig\n",
    "import datawig\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(data)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['Pclass','SibSp','Parch'], # column(s) containing information about the column we want to impute\n",
    "    output_column= 'Age', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "imputed = imputer.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do All Data Cleaning on Only Train Set and Apply Calculations to Validation/Test Later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_ml_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://miro.medium.com/max/700/1*_RA3mCS30Pr0vUxbp25Yxw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Category Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_category_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Rescaling: Standardise/Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Other Distribution Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO LOOK INTO NEXT PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ENCODER LOOK INTO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS FOR ALL PREPROCESSING STEPS TO FEED INTO PIPELINES\n",
    "\n",
    "imputers\n",
    "scaler\n",
    "pca\n",
    "smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
