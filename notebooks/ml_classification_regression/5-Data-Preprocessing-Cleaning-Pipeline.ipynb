{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filepath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(relative_filepath)\n",
    "\n",
    "# for path in sys.path:\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled combined data\n",
    "X_y_data = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_data.pkl\")\n",
    "\n",
    "# Read in pickled train data\n",
    "X_y_train = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_train.pkl\")\n",
    "\n",
    "# Read in pickled test data\n",
    "X_y_test = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_test.pkl\")\n",
    "\n",
    "# Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_ml_missing_data = json.load(open(relative_filepath + \"reports/dicts/dict_ml_missing_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values for config dict\n",
    "input_dfs = [X_y_data,\n",
    "             X_y_train,\n",
    "             X_y_test]\n",
    "\n",
    "target = \"classLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Checklist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.justintodata.com/data-cleaning-python-ultimate-guide/\n",
    "\n",
    "Table Of Contents\n",
    "Missing data\n",
    "Irregular data (Outliers)\n",
    "Unnecessary data\n",
    "Unnecessary type #1: Uninformative / Repetitive\n",
    "Unnecessary type #2: Irrelevant\n",
    "Unnecessary type #3: Duplicates\n",
    "Inconsistent data\n",
    "Inconsistent type #1: Capitalization\n",
    "Inconsistent type #2: Formats\n",
    "Inconsistent type #3: Categorical Values\n",
    "Inconsistent type #4: Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://elitedatascience.com/data-cleaning\n",
    "\n",
    "Remove Unwanted observations\n",
    "    Duplicate observations\n",
    "    Irrelevant observations\n",
    "    \n",
    "Fix Structural Errors\n",
    "\n",
    "Filter Unwanted Outliers\n",
    "\n",
    "Handle Missing Data\n",
    "    Missing categorical data\n",
    "    Missing numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display HTML\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "titanic_X_y_train = pd.read_csv('train.csv')\n",
    "titanic_X_y_test = pd.read_csv('test.csv')\n",
    "\n",
    "# titanic_X_y_train = sns.load_dataset('titanic')\n",
    "# titanic_X_y_train.head()\n",
    "\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train & test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                     y,\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "#                                                     random_state=11)\n",
    "\n",
    "X_train = titanic_X_y_train.drop(target, axis=1)\n",
    "y_train = titanic_X_y_train[target]\n",
    "\n",
    "X_train.head()\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# determine categorical and numerical features\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "print(list(numerical_cols))\n",
    "print(list(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalised preprocessing handlers\n",
    "\n",
    "# Numeric  handlers\n",
    "def num_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "def outlier_handler(X):\n",
    "    pass\n",
    "\n",
    "# Categorical handlers\n",
    "def cat_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def label_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def ordinal_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def target_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass\n",
    "\n",
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass\n",
    "\n",
    "def imbalance_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL WITH RELEVANT ##\n",
    "\n",
    "# Column dtypes selector\n",
    "numerical_cols = []\n",
    "# imputation_cols = []\n",
    "# power_transform_cols = []\n",
    "# outlier_cols = []\n",
    "# scaling_cols = []\n",
    "categorical_cols = []\n",
    "text_cols = []\n",
    "\n",
    "# Function transformers for numeric pipeline\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[numerical_cols], validate=False)\n",
    "apply_num_imputations = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_power_transforms = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_outlier_handling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_scaling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_balancing = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for categorical pipeline\n",
    "get_categorical_data = FunctionTransformer(lambda x: x[categorical_cols], validate=False)\n",
    "apply_cat_imputations = FunctionTransformer(FUNCTION, validate=False) #SimpleImputer(strategy='most_frequent', fill_value='categorical', missing_values=np.nan)\n",
    "apply_label_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_one_hot_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_ordinal_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for text pipeline\n",
    "get_text_data = FunctionTransformer(lambda x: x[text_cols], validate=False)\n",
    "apply_vectorizer = FunctionTransformer(FUNCTION, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual dtype pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('selector', get_numeric_data),\n",
    "    ('imputer', apply_num_imputations),\n",
    "    ('power_transformer', apply_power_transforms),\n",
    "    ('outliers', apply_outlier_handling)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('selector', get_categorical_data),\n",
    "    ('imputer', apply_cat_imputations),\n",
    "    ('le', apply_label_encoding),\n",
    "    ('ohe', apply_one_hot_encoding),\n",
    "    ('ordinal', apply_ordinal_encoding)\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('selector', get_text_data),\n",
    "    ('vectorizer', apply_vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with feature union\n",
    "preprocessor_pl = FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_transformer),\n",
    "        ('categorical', categorical_transformer),\n",
    "        ('text', text_transformer)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "preprocessor_pl = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_transformer),\n",
    "        ('categorical', categorical_transformer),\n",
    "        ('text', text_transformer)\n",
    "    ])),\n",
    "    \n",
    "#     ('scaler', apply_scaling),\n",
    "#     ('imbalance', apply_balancing),\n",
    "#     ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with column transformer\n",
    "preprocessor_pl = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', preprocessor_pl),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')   \n",
    "\n",
    "# Defining an example pipeline\n",
    "model = Pipeline([('transformer', FunctionTransformer(lambda x: 2*x)), ('clf', LogisticRegression())])\n",
    "\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://assets.datacamp.com/production/repositories/4983/datasets/238dde66d8af1b7ebd8ffe82de9df60ad6a68d22/preprocessing3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery for Building Preprocessing Handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from fancyimpute import KNN, IterativeImputer\n",
    "\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "num_imputation_cols =[]\n",
    "\n",
    "num_imputation_handler = ColumnTransformer(transformers=[\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "num_imputation_handler.fit_transform(X_train)\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "\n",
    "# Options: mean/median/mode/constant\n",
    "imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imputer_constant = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "\n",
    "X_train.fillna(method='ffill', inplace=True)\n",
    "X_train.fillna(method='bfill', inplace=True)\n",
    "\n",
    "X_train.interpolate(method='linear', inplace=True)\n",
    "X_train.interpolate(method='quadratic', inplace=True)\n",
    "X_train.interpolate(method='nearest', inplace=True)\n",
    "\n",
    "fancyimpute KNN, MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "power_transform_cols = ['Age', 'Fare']\n",
    "\n",
    "power_transform = PowerTransformer(method='yeo-johnson',\n",
    "                                   standardize=False)\n",
    "\n",
    "power_transform_handler = ColumnTransformer(transformers=[\n",
    "    ('pt', power_transform, power_transform_cols),\n",
    "], remainder='drop', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "print('X Shape before PT:', X_train.shape, '\\n')\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "X_train_pt = power_transform_handler.fit_transform(X_train)\n",
    "print('X Shape after PT as CT numpy:', X_train_pt.shape, '\\n')\n",
    "# print(X_train_pt, '\\n')\n",
    "\n",
    "# # Pdf for verification / exploration\n",
    "X_train_pt = pd.DataFrame(X_train_pt, columns=power_transform_handler.get_feature_names_out())\n",
    "X_train_pt.dtypes\n",
    "X_train_pt.head()\n",
    "\n",
    "# histograms of the features before power transforms\n",
    "X_train[numerical_cols].hist()\n",
    "# histograms of the features after power transforms\n",
    "X_train_pt[power_transform_cols].hist()\n",
    "# plt.show()\n",
    "\n",
    "# Options:\n",
    "# # Yeo-Johnson supports both positive or negative data\n",
    "# pt = PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "# # Box-Cox requires input data to be strictly positive\n",
    "# pt = PowerTransformer(method='box-cox')\n",
    "\n",
    "# In SciPy:\n",
    "# y, fitted_lambda = yeojohnson(y, lmbda=None)\n",
    "# y, fitted_lambda = boxcox(y, lmbda=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/an-easy-tool-to-correctly-transform-non-linear-data-for-linear-regression-5fbe7f7bfe2f\n",
    "\n",
    "Image(url = \"https://miro.medium.com/max/1170/1*iiTwCk-QjOTS83Rl1qfO7A.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The goal of this article is to demonstrate how to use this diagram to make transformations to your data. Before we get into examples, the way you can read this diagram is:\n",
    "\n",
    "If you see data that looks like the curve in the top left, \n",
    "    you can try to decrease the power of x and/or increase the power of y.\n",
    "    \n",
    "If you see data that looks like the curve in the top right, \n",
    "    you can try to increase the power of x and/or increase the power of y.\n",
    "    \n",
    "If you see data that looks like the curve in the bottom right, \n",
    "    you can try to increase the power of x and/or decrease the power of y.\n",
    "    \n",
    "If you see data that looks like the curve in the bottom left, \n",
    "    you can try to decrease the power of x and/or decrease the power of y.\n",
    "\n",
    "Notes:\n",
    "    Squaring the input variable is great for modeling data that are better fit by a curved line.\n",
    "        transforming X: do not change the values of the residuals\n",
    "    \n",
    "    Using the log transform on the response is good for data where the variance is unequal.\n",
    "        transforming y: the relationship between the linear model and the error terms is also changed.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/872/1*Jwpotn5OKYfkzoGQFYKunA.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the data are right-skewed (clustered at lower values) \n",
    "    move down the ladder of powers \n",
    "    (that is, try square root, cube root, logarithmic, etc. transformations).\n",
    "    \n",
    "If the data are left-skewed (clustered at higher values) \n",
    "    move up the ladder of powers \n",
    "    (cube, square, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/656/1*8jUUiaF9dD9ZiLzH8e_9jA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1400/1*RRZ4lakWAhBWRMC9r1r0Ew.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The boxcox() SciPy function implements the Box-Cox method. It takes an argument, called lambda, that controls the type of transform to perform.\n",
    "\n",
    "lambda = -1. is a reciprocal transform.\n",
    "lambda = -0.5 is a reciprocal square root transform.\n",
    "lambda = 0.0 is a log transform.\n",
    "lambda = 0.5 is a square root transform.\n",
    "lambda = 1.0 is no transform.\n",
    "\n",
    "A limitation of the Box-Cox transform is that it assumes that all values in the data sample are positive.\n",
    "\n",
    "Yeo-Johnson Transformation Method\n",
    "\n",
    "Unlike the Box-Cox transform, it does not require the values for each input variable to be strictly positive.\n",
    "It supports zero values and negative values. This means we can apply it to our dataset without scaling it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "outlier_cols =\n",
    "\n",
    "outlier_handler = ColumnTransformer(transformers=[\n",
    "    ('outlier_remover', OutlierRemover(), outlier_cols)\n",
    "], remainder='passthrough')\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "#Load data\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "#Create data frame\n",
    "boston = load_boston()\n",
    "columns = boston.feature_names\n",
    "df = pd.DataFrame(X, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()\n",
    "\n",
    "df_1 = df[['TAX', 'B']]\n",
    "df_2 = df[['CRIM', 'ZN', 'INDUS', 'RM', 'AGE', 'DIS', 'RAD', 'PTRATIO','LSTAT']]\n",
    "df_3 = df[['CHAS', 'NOX']]\n",
    "\n",
    "ax = sns.boxplot(data=df_2, orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-1-4ece5098b755\n",
    "\n",
    "Tukey’s box plot method:\n",
    "\n",
    "Next to its visual benefits, the box plot provides useful statistics to identify individual observations as outliers. \n",
    "\n",
    "Tukey distinguishes between possible and probable outliers. \n",
    "A possible outlier is located between the inner and the outer fence, whereas a probable outlier is located outside the outer fence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1342/1*vQyvZ7yZpLcFk7eDdoc5lg.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://miro.medium.com/max/1342/1*vQyvZ7yZpLcFk7eDdoc5lg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tukey's method\n",
    "def tukeys_method(df, variable):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    q1 = df[variable].quantile(0.25)\n",
    "    q3 = df[variable].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    inner_fence = 1.5*iqr\n",
    "    outer_fence = 3*iqr\n",
    "    \n",
    "    #inner fence lower and upper end\n",
    "    inner_fence_le = q1-inner_fence\n",
    "    inner_fence_ue = q3+inner_fence\n",
    "    \n",
    "    #outer fence lower and upper end\n",
    "    outer_fence_le = q1-outer_fence\n",
    "    outer_fence_ue = q3+outer_fence\n",
    "    \n",
    "    outliers_prob = []\n",
    "    outliers_poss = []\n",
    "    for index, x in enumerate(df[variable]):\n",
    "        if x <= outer_fence_le or x >= outer_fence_ue:\n",
    "            outliers_prob.append(index)\n",
    "    for index, x in enumerate(df[variable]):\n",
    "        if x <= inner_fence_le or x >= inner_fence_ue:\n",
    "            outliers_poss.append(index)\n",
    "    return outliers_prob, outliers_poss\n",
    "        \n",
    "probable_outliers_tm, possible_outliers_tm = tukeys_method(df, \"CRIM\")\n",
    "\n",
    "print(probable_outliers_tm)\n",
    "# [374, 375, 376, 378, 379, 380, 381, 384, 385, 386, 387, 398, 400, 403, 404, 405, 406, \n",
    "# 410 412, 413, 414, 415, 417, 418, 425, 427, 437, 440, 468, 477]\n",
    "print(possible_outliers_tm)\n",
    "# [367, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 384, 385, 386, 387, 388, \n",
    "# 392, 394, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 409, 410, 411, 412, 413, \n",
    "# 414, 415, 416, 417, 418, 419, 420, 422, 425, 426, 427, 429, 431, 434, 435, 436, 437,\n",
    "# 438, 439, 440, 441, 443, 444, 445, 447, 448, 454, 468, 469, 477, 478, 479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform 'CRIM' to log \n",
    "log_CRIM = np.log(df['CRIM'])\n",
    "df['CRIM_man'] = df['CRIM']+1\n",
    "log_CRIM = np.log(df['CRIM_man'])\n",
    "df['CRIM_log'] = log_CRIM\n",
    "\n",
    "#Plot\n",
    "sns.distplot(df['CRIM_log'])\n",
    "\n",
    "#Calculate probable and possible outliers using log-iq method\n",
    "probable_outliers_logiq, possible_outliers_logiq = tukeys_method(df, 'CRIM_log')\n",
    "print(probable_outliers_logiq)\n",
    "print(possible_outliers_logiq)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Following a common rule of thumb, if z > C, where C is usually set to 3, the observation is marked as an outlier. This rule stems from the fact that if a variable is normally distributed, 99.7% of all data points are located 3 standard deviations around the mean. Let’s see on our example, which observations of ‘CRIM’ are detected to be outliers using the z-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internally studentized method (z-score)\n",
    "def z_score_method(df, variable_name):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    columns = df.columns\n",
    "    z = np.abs(stats.zscore(df))\n",
    "    threshold = 3\n",
    "    outlier = []\n",
    "    index=0\n",
    "    for item in range(len(columns)):\n",
    "        if columns[item] == variable_name:\n",
    "            index = item\n",
    "    for i, v in enumerate(z[:, index]):\n",
    "        if v > threshold:\n",
    "            outlier.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier\n",
    "\n",
    "outlier_z = z_score_method(df, 'CRIM')\n",
    "print(outlier_z)\n",
    "# [380, 398, 404, 405, 410, 414, 418, 427]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When using the z-score method, 8 observations are marked as outliers. However, this method is highly limited as the distributions mean and standard deviation are sensitive to outliers. This means that finding one outlier is dependent on other outliers as every observation directly affects the mean.\n",
    "\n",
    "Moreover, the z-score method assumes the variable of interest to be normally distributed. A more robust method that can be used instead is the externally studentized residuals. Here, the influence of the examined data point is removed from the calculation of the mean and standard deviation, like so:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The test statistic is calculated like the z-score using robust statistics. Also, to identify outlying observations, the same cut-off point of 3 is used. If the test statistic lies above 3, it is marked as an outlier. Compared to the internally (z-score) and externally studentized residuals, this method is more robust to outliers and does assume X to be parametrically distributed (Examples of discrete and continuous parametric distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAD method\n",
    "def mad_method(df, variable_name):\n",
    "    #Takes two parameters: dataframe & variable of interest as string\n",
    "    columns = df.columns\n",
    "    med = np.median(df, axis = 0)\n",
    "    mad = np.abs(stats.median_absolute_deviation(df))\n",
    "    threshold = 3\n",
    "    outlier = []\n",
    "    index=0\n",
    "    for item in range(len(columns)):\n",
    "        if columns[item] == variable_name:\n",
    "            index == item\n",
    "    for i, v in enumerate(df.loc[:,variable_name]):\n",
    "        t = (v-med[index])/mad[index]\n",
    "        if t > threshold:\n",
    "            outlier.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier\n",
    "\n",
    "outlier_mad = mad_method(df, 'CRIM')\n",
    "print(outlier_mad)\n",
    "#[20, 31, 32, 34, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155,\n",
    "# 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 310, 356, 357, \n",
    "# 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, \n",
    "# 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, \n",
    "# 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, \n",
    "# 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, \n",
    "# 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, \n",
    "# 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, \n",
    "# 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, \n",
    "# 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-2-3a3319ec2c33"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Handling non-error outliers\n",
    "\n",
    "There exist three different options on how to treat non-error outliers:\n",
    "Keep\n",
    "Delete\n",
    "Recode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mahalonibis Distance\n",
    "\n",
    "def mahalanobis_method(df):\n",
    "    #M-Distance\n",
    "    x_minus_mu = df - np.mean(df)\n",
    "    cov = np.cov(df.values.T)                           #Covariance\n",
    "    inv_covmat = sp.linalg.inv(cov)                     #Inverse covariance\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat) \n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    #Cut-off point\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))    #degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "outliers_mahal_bi, md_bi = mahalanobis_method(df=df_bivariate)\n",
    "#[380, 398, 404, 405, 410, 414, 418, 427]\n",
    "\n",
    "outliers_mahal, md = mahalanobis_method(df=df)\n",
    "#[152, 155, 214, 353, 364, 365, 367, 380, 405, 410, 414, 418, 488, 489, 490, 491, 492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robust Mahalonibis Distance\n",
    "def robust_mahalanobis_method(df):\n",
    "    #Minimum covariance determinant\n",
    "    rng = np.random.RandomState(0)\n",
    "    real_cov = np.cov(df.values.T)\n",
    "    X = rng.multivariate_normal(mean=np.mean(df, axis=0), cov=real_cov, size=506)\n",
    "    cov = MinCovDet(random_state=0).fit(X)\n",
    "    mcd = cov.covariance_ #robust covariance metric\n",
    "    robust_mean = cov.location_  #robust mean\n",
    "    inv_covmat = sp.linalg.inv(mcd) #inverse covariance metric\n",
    "    \n",
    "    #Robust M-Distance\n",
    "    x_minus_mu = df - robust_mean\n",
    "    left_term = np.dot(x_minus_mu, inv_covmat)\n",
    "    mahal = np.dot(left_term, x_minus_mu.T)\n",
    "    md = np.sqrt(mahal.diagonal())\n",
    "    \n",
    "    #Flag as outlier\n",
    "    outlier = []\n",
    "    C = np.sqrt(chi2.ppf((1-0.001), df=df.shape[1]))#degrees of freedom = number of variables\n",
    "    for index, value in enumerate(md):\n",
    "        if value > C:\n",
    "            outlier.append(index)\n",
    "        else:\n",
    "            continue\n",
    "    return outlier, md\n",
    "\n",
    "outliers_mahal_rob_bi, md_rb_bi = robust_mahalanobis_method(df=df_bivariate)\n",
    "#[141, 374, 380, 398, 404, 405, 410, 414, 418, 427]\n",
    "\n",
    "outliers_mahal_rob, md_rb = robust_mahalanobis_method(df=df)\n",
    "#[123, 126, 142, 152, 155, 163, 214, 283, 353, 364, 365, 367, 380, 405, 410, \n",
    "# 418, 488, 489, 490, 491, 492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "#You need deep copy otherwise cannot \n",
    "#add column to a slice of a DataFrame\n",
    "df_bi_cp = copy.deepcopy(df_bivariate) \n",
    "\n",
    "#Add md and robust md to copy of dataframe\n",
    "df_bi_cp['md'] = md_bi\n",
    "df_bi_cp['md_robust'] = md_rb_bi\n",
    "\n",
    "def flag_outliers(df, outliers):\n",
    "    flag = []\n",
    "    for index in range(df.shape[0]):\n",
    "        if index in outliers:\n",
    "            flag.append(1)\n",
    "        else:\n",
    "            flag.append(0)\n",
    "    return flag\n",
    "\n",
    "#Flag outliers with 1, others with 0\n",
    "df_bi_cp['flag'] = flag_outliers(df_bivariate, outliers_mahal_bi)\n",
    "df_bi_cp['flag_rob'] = flag_outliers(df_bivariate, outliers_mahal_rob_bi)\n",
    "\n",
    "#MD classic\n",
    "ax = sns.scatterplot(x=\"LSTAT\", y=\"CRIM\", hue='flag', data=df_bi_cp)\n",
    "#MD robust\n",
    "ax = sns.scatterplot(x=\"LSTAT\", y=\"CRIM\", hue='flag_rob', data=df_bi_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/detecting-and-treating-outliers-in-python-part-3-dcb54abaf7b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class OutlierRemover(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,factor=1.5):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def outlier_detector(self,X,y=None):\n",
    "        X = pd.Series(X).copy()\n",
    "        q1 = X.quantile(0.25)\n",
    "        q3 = X.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        self.lower_bound.append(q1 - (self.factor * iqr))\n",
    "        self.upper_bound.append(q3 + (self.factor * iqr))\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.lower_bound = []\n",
    "        self.upper_bound = []\n",
    "        X.apply(self.outlier_detector)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for i in range(X.shape[1]):\n",
    "            x = X.iloc[:, i].copy()\n",
    "            x[(x < self.lower_bound[i]) | (x > self.upper_bound[i])] = np.nan\n",
    "            X.iloc[:, i] = x\n",
    "        return X\n",
    "    \n",
    "outlier_remover = OutlierRemover()\n",
    "\n",
    "test = pd.DataFrame({'col1':[100,200,300,999],'col2':[0,0,1,2],'col3':[-10,0,1,2]})\n",
    "test\n",
    "\n",
    "outlier_remover.fit_transform(test)\n",
    "\n",
    "#\n",
    "data.plot(kind=\"box\",subplots=True,figsize=(15,5),title=\"Data with Outliers\");\n",
    "\n",
    "#\n",
    "outlier_remover = OutlierRemover()\n",
    "\n",
    "#ColumnTransformer to remove outliers\n",
    "ct = ColumnTransformer(transformers=[['outlier_remover',OutlierRemover(),list(range(data.shape[1]))]],remainder='passthrough')\n",
    "\n",
    "#iris data after outlier removal\n",
    "data_without_outliers = pd.DataFrame(ct.fit_transform(data),columns=data.columns)\n",
    "\n",
    "#iris data box plot after outlier removal\n",
    "data_without_outliers.plot(kind=\"box\",subplots=True,figsize=(15,5),title=\"Data without Outliers\");\n",
    "\n",
    "# 4 outliers are removed from SepalWidthCm, other columns stayed the same as they have no outliers.\n",
    "data_without_outliers.isnull().sum()\n",
    "\n",
    "#outliers removed from sepal width (cm)\n",
    "list(data.loc[data_without_outliers.isnull().sum(axis=1)>0,'SepalWidthCm'])\n",
    "\n",
    "# Method 2\n",
    "def outlier_removal(X,factor):\n",
    "    X = pd.DataFrame(X).copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        x = pd.Series(X.iloc[:,i]).copy()\n",
    "        q1 = x.quantile(0.25)\n",
    "        q3 = x.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (factor * iqr)\n",
    "        upper_bound = q3 + (factor * iqr)\n",
    "        X.iloc[((X.iloc[:,i] < lower_bound) | (X.iloc[:,i] > upper_bound)),i] = np.nan \n",
    "    return X\n",
    "\n",
    "#creating outlier_remover object using FunctionTransformer with factor=1.5\n",
    "outlier_remover = FunctionTransformer(outlier_removal,kw_args={'factor':1.5})\n",
    "\n",
    "test = pd.DataFrame({'col1':[100,200,300,999],'col2':[0,0,1,2],'col3':[-10,0,1,2]})\n",
    "test\n",
    "\n",
    "outlier_remover.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Pclass       891 non-null    int64  \n",
      " 2   Name         891 non-null    object \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          714 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Ticket       891 non-null    object \n",
      " 8   Fare         891 non-null    float64\n",
      " 9   Cabin        204 non-null    object \n",
      " 10  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 76.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.info()\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/891 with 1 missing, elapsed time: 0.042\n",
      "Imputing row 101/891 with 1 missing, elapsed time: 0.044\n",
      "Imputing row 201/891 with 1 missing, elapsed time: 0.045\n",
      "Imputing row 301/891 with 1 missing, elapsed time: 0.045\n",
      "Imputing row 401/891 with 1 missing, elapsed time: 0.046\n",
      "Imputing row 501/891 with 1 missing, elapsed time: 0.047\n",
      "Imputing row 601/891 with 1 missing, elapsed time: 0.048\n",
      "Imputing row 701/891 with 0 missing, elapsed time: 0.049\n",
      "Imputing row 801/891 with 1 missing, elapsed time: 0.050\n",
      "[KNN] Warning: 687/891 still missing after imputation, replacing with 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cabin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cabin\n",
       "0   0.0\n",
       "1  81.0\n",
       "2   0.0\n",
       "3  55.0\n",
       "4   0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fancyimpute import KNN, IterativeImputer\n",
    "\n",
    "def cat_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "# cat_imputation_handler =\n",
    "####\n",
    "####\n",
    "\n",
    "# Create dictionary for ordinal encoders\n",
    "cat_imputation_cols = ['Cabin']\n",
    "ordinal_enc_dict = {}\n",
    "\n",
    "# Loop over columns to encode\n",
    "for col_name in X_train[cat_imputation_cols]:\n",
    "    #Create ordinal encoder for the column\n",
    "    ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "    \n",
    "    # Select the non-null values in the column\n",
    "    col = X_train[col_name]\n",
    "    col_not_null = col[col.notnull()]\n",
    "    reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "    \n",
    "    # Encode the non-null values of the column\n",
    "    encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Replace the column with ordinal values\n",
    "    X_train.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)\n",
    "\n",
    "X_train_KNN_imputed = X_train[cat_imputation_cols].copy(deep=True)\n",
    "X_train_KNN_imputed.head()\n",
    "# Create KNN imputer\n",
    "KNN_imputer = KNN()\n",
    "\n",
    "X_train_KNN_imputed.iloc[:, :] = np.round(KNN_imputer.fit_transform(X_train_KNN_imputed))\n",
    "\n",
    "for col in X_train_KNN_imputed:\n",
    "    reshaped_col = X_train_KNN_imputed[col].values.reshape(-1, 1)\n",
    "    X_train_KNN_imputed[col] = ordinal_enc_dict[col].inverse_transform(reshaped_col)\n",
    "\n",
    "X_train_KNN_imputed.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate both packages to use\n",
    "encoder = OrdinalEncoder()\n",
    "imputer = IterativeImputer(ExtraTreesRegressor())\n",
    "imputer = KNN()\n",
    "\n",
    "# create a list of categorical columns to iterate over\n",
    "cat_imputation_cols = []\n",
    "\n",
    "def encode(data):\n",
    "    '''function to encode non-null data and replace it in the original data'''\n",
    "    #retains only non-null values\n",
    "    nonulls = np.array(data.dropna())\n",
    "    #reshapes the data for encoding\n",
    "    impute_reshape = nonulls.reshape(-1,1)\n",
    "    #encode date\n",
    "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
    "    #Assign back encoded values to non-null values\n",
    "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
    "    return data\n",
    "\n",
    "#create a for loop to iterate through each column in the data\n",
    "for columns in cat_imputation_cols:\n",
    "    encode(impute_data[columns])\n",
    "    \n",
    "impute_data\n",
    "\n",
    "# impute data and convert \n",
    "encode_data = pd.DataFrame(np.round(imputer.fit_transform(impute_data)), columns = impute_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding_handler(y):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "label_encoding_handler = LabelEncoder()\n",
    "####\n",
    "####\n",
    "\n",
    "print('y Shape before LE:', y_train.shape, '\\n')\n",
    "\n",
    "# Label encode target variable output as numpy array\n",
    "y_train_le = label_encoding_handler.fit_transform(y_train) \n",
    "print('y Shape after LE as numpy:', y_train.shape, '\\n')\n",
    "# print(y_train_le, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "ohe_cols = ['Sex', 'Embarked']\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "\n",
    "one_hot_encoding_handler = ColumnTransformer(transformers=[\n",
    "    ('ohe', ohe, ohe_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "print('X Shape before OHE:', X_train.shape, '\\n')\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "X_train_ohe = one_hot_encoding_handler.fit_transform(X_train)\n",
    "print('X Shape after OHE as CT numpy:', X_train_ohe.shape, '\\n')\n",
    "print(X_train_ohe, '\\n')\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe, columns=one_hot_encoding_handler.get_feature_names_out())\n",
    "print('X Shape after OHE as pdf:', X_train_ohe.shape, '\\n')\n",
    "# X_train_ohe.dtypes\n",
    "X_train_ohe.head()\n",
    "# Options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encoding_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "ordinal_cols = ['Pclass']\n",
    "\n",
    "ordinal_feat_1_categories = [1, 2, 3]\n",
    "# ordinal_feat_2_categories = ['first', 'second', 'third']\n",
    "\n",
    "ordinal = OrdinalEncoder(categories=[ordinal_feat_1_categories])\n",
    "\n",
    "ordinal_encoding_handler = ColumnTransformer(transformers=[\n",
    "    ('ordinal', ordinal, ordinal_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "print('X Shape before Ordinal Encoding:', X_train_ohe.shape, '\\n')\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "X_train_ordinal = ordinal_encoding_handler.fit_transform(X_train_ohe)\n",
    "print('X Shape after Ordinal Encoding as CT numpy:', X_train_ordinal.shape, '\\n')\n",
    "print(X_train_ordinal, '\\n')\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "X_train_ordinal = pd.DataFrame(X_train_ordinal, columns=ordinal_cols + list(X_train_ohe.drop(columns=ordinal_cols).columns))\n",
    "X_train_ordinal.columns = [col +'_ordinal' if col in ordinal_cols else col for col in X_train_ordinal.columns]\n",
    "# X_train_ordinal.dtypes\n",
    "X_train_ordinal.head()\n",
    "\n",
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "target_encoding_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "vectorizer_handler =\n",
    "####\n",
    "####\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "\n",
    "\n",
    "# Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model input transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "scaling_handler =\n",
    "####\n",
    "####\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "\n",
    "\n",
    "# Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "imbalance_handler =\n",
    "####\n",
    "####\n",
    "\n",
    "# Numpy implementation for pipelines\n",
    "\n",
    "\n",
    "# Pdf for verification / exploration\n",
    "\n",
    "\n",
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "###################\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# \n",
    "from sklearn import set_config                      # to change the display\n",
    "from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation / Scoring metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y = True, as_frame=True)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "X.drop('CHAS', axis=1, inplace=True)\n",
    "y = pd.Series(boston.target, name='MEDV')\n",
    "\n",
    "# inspect data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "X_train.shape\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = X_train.select_dtypes(include=['object']).columns\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# cat_features = []\n",
    "# num_features = []\n",
    "\n",
    "# print(cat_features)\n",
    "# print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_features)\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing.outliers import boxplot_numeric_features, IQR_Outliers, CustomSampler_IQR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numeric Outlier (IQR)\n",
    "Z-Score\n",
    "DBSCAN\n",
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "rows_for_plot = 6\n",
    "cols_for_plot = 5\n",
    "###\n",
    "\n",
    "boxplot_numeric_features(X_train,\n",
    "                         rows_for_plot=rows_for_plot,\n",
    "                         cols_for_plot=cols_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before IQR outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after IQR outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before Z-score outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after Z-score outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR_Outliers(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomSampler_IQR(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# # select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
    "\n",
    "# # evaluate the model\n",
    "# yhat = model.predict(X_test)\n",
    "\n",
    "# # evaluate predictions\n",
    "# mae = mean_absolute_error(y_test, yhat)\n",
    "# print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "# Do i want to remove outliers from test dataset? Without the outlier tows then no prediction can be made\n",
    "LR_Pipeline = Pipeline([\n",
    "    ('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False)),\n",
    "    ('Imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat_transformer', cat_pipe, cat_features),\n",
    "    ('num_transformer', num_pipe, num_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict training data\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "# print(f\"Predictions on training data: {y_train_pred}\")\n",
    "print(\"Accuracy on Training set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "# print(f\"Predictions on test data: {y_test_pred}\")\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config to diagram for visualizing the pipelines/composite estimators\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Lets visualize the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Num_vars is the list of numerical variables \n",
    "X_train_num = X_train[numeric_features]\n",
    "X_train_num = imputer.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_train_cat_ord_encoded = ordinal_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_ord_encoded[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "X_train_cat_hot_encoded = cat_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome transformations\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ratings_index = -2\n",
    "reviews_index = -1\n",
    "class NewVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "    # Make a new variable that is rating divided by number of reviews\n",
    "        ratings_over_reviews = X[:,ratings_index]/X[:,reviews_index]\n",
    "        return np.c_[X, ratings_over_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('add_variables', NewVariablesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_num_transformed = num_pipeline.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, num_vars),\n",
    "    ('categorical', OneHotEncoder(), cat_vars),\n",
    "    \n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('features', FeatureUnion ([\n",
    "     ('Cat Columns', Pipeline([\n",
    "          ('Category Extractor', TypeSelector(np.number)),\n",
    "                 ('Impute Zero', SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "                                    ])),\n",
    "('Numerics', Pipeline([\n",
    "      ('Numeric Extractor', TypeSelector(\"category\")),\n",
    "          ('Impute Missing', SimpleImputer(strategy=\"constant\", fill_value='missing'))\n",
    "          ]))        \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median', fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "print (\"## Number of duplicate rows ## \\n\")\n",
    "if len(X_y_data[X_y_data.duplicated()]) > 0:\n",
    "    print(\"Number of duplicated observations: \", len(X_y_data[X_y_data.duplicated()]))\n",
    "    X_y_data[X_y_data.duplicated(keep=False)].sort_values(by=list(X_y_data.columns)).head()\n",
    "else:\n",
    "    print(\"No duplicated observations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_y_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with a lot of missing values.\n",
    "ind_missing = df[df['num_missing'] > 35].index\n",
    "df_less_missing_rows = df.drop(ind_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_beds_raion has a lot of missing.\n",
    "# If we want to drop.\n",
    "cols_to_drop = ['hospital_beds_raion']\n",
    "df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with the median.\n",
    "med = df['life_sq'].median()\n",
    "print(med)\n",
    "df['life_sq'] = df['life_sq'].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each numeric column.\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        med = df[col].median()\n",
    "        df[col] = df[col].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each non-numeric column.\n",
    "df_non_numeric = df.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = df[col].describe()['top'] # impute with the most frequent value.\n",
    "        df[col] = df[col].fillna(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "df['sub_area'] = df['sub_area'].fillna('_MISSING_')\n",
    "\n",
    "\n",
    "# numeric\n",
    "df['life_sq'] = df['life_sq'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This article covers 7 ways to handle missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deleting Rows with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for continuous variable\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].mean())\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other Imputation Methods\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].fillna(method='ffill')\n",
    "data[\"Age\"] = data[\"Age\"].interpolate(method='linear', limit_direction='forward', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Algorithms that support missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction of missing values\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]]\n",
    "\n",
    "data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]\n",
    "\n",
    "test_data = data[data[\"Age\"].isnull()]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "y_train = data[\"Age\"]\n",
    "X_train = data.drop(\"Age\", axis=1)\n",
    "X_test = test_data.drop(\"Age\", axis=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation using Deep Learning Library — Datawig\n",
    "\n",
    "import pandas as pd\n",
    "#pip install datawig\n",
    "import datawig\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(data)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['Pclass','SibSp','Parch'], # column(s) containing information about the column we want to impute\n",
    "    output_column= 'Age', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "imputed = imputer.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do All Data Cleaning on Only Train Set and Apply Calculations to Validation/Test Later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_ml_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://miro.medium.com/max/700/1*_RA3mCS30Pr0vUxbp25Yxw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Category Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_category_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Rescaling: Standardise/Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Other Distribution Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO LOOK INTO NEXT PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ENCODER LOOK INTO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS FOR ALL PREPROCESSING STEPS TO FEED INTO PIPELINES\n",
    "\n",
    "imputers\n",
    "scaler\n",
    "pca\n",
    "smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
