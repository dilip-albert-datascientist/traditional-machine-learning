{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show ALL outputs in cell, not only last result\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filepath = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set relative path mapping for module imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(relative_filepath)\n",
    "\n",
    "# for path in sys.path:\n",
    "#     print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pickled combined data\n",
    "X_y_data = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_data.pkl\")\n",
    "\n",
    "# Read in pickled train data\n",
    "X_y_train = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_train.pkl\")\n",
    "\n",
    "# Read in pickled test data\n",
    "X_y_test = pd.read_pickle(relative_filepath + \"data/interim/step_3a/X_y_test.pkl\")\n",
    "\n",
    "# Recap data structure\n",
    "X_y_data.head()\n",
    "X_y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dict_ml_missing_data = json.load(open(relative_filepath + \"reports/dicts/dict_ml_missing_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values for config dict\n",
    "input_dfs = [X_y_data,\n",
    "             X_y_train,\n",
    "             X_y_test]\n",
    "\n",
    "target = \"classLabel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Checklist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.justintodata.com/data-cleaning-python-ultimate-guide/\n",
    "\n",
    "Table Of Contents\n",
    "Missing data\n",
    "Irregular data (Outliers)\n",
    "Unnecessary data\n",
    "Unnecessary type #1: Uninformative / Repetitive\n",
    "Unnecessary type #2: Irrelevant\n",
    "Unnecessary type #3: Duplicates\n",
    "Inconsistent data\n",
    "Inconsistent type #1: Capitalization\n",
    "Inconsistent type #2: Formats\n",
    "Inconsistent type #3: Categorical Values\n",
    "Inconsistent type #4: Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://elitedatascience.com/data-cleaning\n",
    "\n",
    "Remove Unwanted observations\n",
    "    Duplicate observations\n",
    "    Irrelevant observations\n",
    "    \n",
    "Fix Structural Errors\n",
    "\n",
    "Filter Unwanted Outliers\n",
    "\n",
    "Handle Missing Data\n",
    "    Missing categorical data\n",
    "    Missing numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "titanic_X_y_train = pd.read_csv('train.csv')\n",
    "titanic_X_y_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                               Name  \\\n",
       "0            1       3                            Braund, Mr. Owen Harris   \n",
       "1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3       3                             Heikkinen, Miss. Laina   \n",
       "3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5       3                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train & test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "#                                                     y,\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "#                                                     random_state=11)\n",
    "\n",
    "X_train = titanic_X_y_train.drop(\"Survived\", axis=1)\n",
    "y_train = titanic_X_y_train[\"Survived\"]\n",
    "\n",
    "X_train.head()\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
      "['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# determine categorical and numerical features\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "print(list(numerical_cols))\n",
    "print(list(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalised preprocessing handlers\n",
    "\n",
    "# Numeric  handlers\n",
    "def num_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "def outlier_handler(X):\n",
    "    pass\n",
    "\n",
    "# Categorical handlers\n",
    "def cat_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "def label_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def ordinal_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "def target_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass\n",
    "\n",
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass\n",
    "\n",
    "def imbalance_handler(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine categorical and numerical features\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'bool']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILL WITH RELEVANT ##\n",
    "\n",
    "# Column dtypes selector\n",
    "numerical_cols = []\n",
    "# imputation_cols = []\n",
    "# power_transform_cols = []\n",
    "# outlier_cols = []\n",
    "# scaling_cols = []\n",
    "categorical_cols = []\n",
    "text_cols = []\n",
    "\n",
    "# Function transformers for numeric pipeline\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[numerical_cols], validate=False)\n",
    "apply_num_imputations = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_power_transforms = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_outlier_handling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_scaling = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_balancing = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for categorical pipeline\n",
    "get_categorical_data = FunctionTransformer(lambda x: x[categorical_cols], validate=False)\n",
    "apply_cat_imputations = FunctionTransformer(FUNCTION, validate=False) #SimpleImputer(strategy='most_frequent', fill_value='categorical', missing_values=np.nan)\n",
    "apply_label_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_one_hot_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "apply_ordinal_encoding = FunctionTransformer(FUNCTION, validate=False)\n",
    "\n",
    "# Function transformers for text pipeline\n",
    "get_text_data = FunctionTransformer(lambda x: x[text_cols], validate=False)\n",
    "apply_vectorizer = FunctionTransformer(FUNCTION, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual dtype pipelines\n",
    "numeric_transformer = Pipeline([\n",
    "    ('selector', get_numeric_data),\n",
    "    ('imputer', apply_num_imputations),\n",
    "    ('power_transformer', apply_power_transforms),\n",
    "    ('outliers', apply_outlier_handling)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('selector', get_categorical_data),\n",
    "    ('imputer', apply_cat_imputations),\n",
    "    ('le', apply_label_encoding),\n",
    "    ('ohe', apply_one_hot_encoding),\n",
    "    ('ordinal', apply_ordinal_encoding)\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('selector', get_text_data),\n",
    "    ('vectorizer', apply_vectorizer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with feature union\n",
    "preprocessor_pl = FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_transformer),\n",
    "        ('categorical', categorical_transformer),\n",
    "        ('text', text_transformer)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "preprocessor_pl = Pipeline([\n",
    "    ('union', FeatureUnion(transformer_list=[\n",
    "        ('numeric', numeric_transformer),\n",
    "        ('categorical', categorical_transformer),\n",
    "        ('text', text_transformer)\n",
    "    ])),\n",
    "    \n",
    "#     ('scaler', apply_scaling),\n",
    "#     ('imbalance', apply_balancing),\n",
    "#     ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipeline with column transformer\n",
    "preprocessor_pl = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "preprocessor_pl_result = preprocessor_pl.fit_transform(X_train)\n",
    "type(preprocessor_pl_result)\n",
    "preprocessor_pl_result.shape\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', preprocessor_pl),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "    'classifier__C': [0.1, 1.0, 10, 100],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10)\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display='diagram')   \n",
    "\n",
    "# Defining an example pipeline\n",
    "model = Pipeline([('transformer', FunctionTransformer(lambda x: 2*x)), ('clf', LogisticRegression())])\n",
    "\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://assets.datacamp.com/production/repositories/4983/datasets/238dde66d8af1b7ebd8ffe82de9df60ad6a68d22/preprocessing3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery for Building Preprocessing Handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "from sklearn.preprocessing import PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "num_imputation_handler = ColumnTransformer(transformers=[\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "    ('imputer', imputer, cols),\n",
    "], \n",
    "                                 remainder='passthrough')\n",
    "####\n",
    "####\n",
    "\n",
    "# Impute mean/median/mode/constant\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "impute_median = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "impute_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "impute_constant = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "\n",
    "imp_mean.fit_transform(X)\n",
    "\n",
    "# Drop cols\n",
    "\n",
    "# Drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_transform_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "power_transform_handler = ColumnTransformer(transformers=[\n",
    "    ('pt', power_transform, cols),\n",
    "], \n",
    "                                 remainder='passthrough')\n",
    "####\n",
    "####\n",
    "\n",
    "# Yeo-Johnson supports both positive or negative data\n",
    "pt = PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
    "data = pt.fit_transform(data)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = DataFrame(data)\n",
    "# histograms of the variables\n",
    "dataset.hist()\n",
    "pyplot.show()\n",
    "\n",
    "# Box-Cox requires input data to be strictly positive\n",
    "# pt = PowerTransformer(method='box-cox')\n",
    "# data = pt.fit_transform(data)\n",
    "\n",
    "# In SciPy:\n",
    "# y, fitted_lambda = yeojohnson(y, lmbda=None)\n",
    "# y, fitted_lambda = boxcox(y, lmbda=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "→ The ladder of powers\n",
    "Data transformations are commonly power transformations, x’=xθ (where x’ is the transformed x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/656/1*8jUUiaF9dD9ZiLzH8e_9jA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/872/1*Jwpotn5OKYfkzoGQFYKunA.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "If the data are right-skewed (clustered at lower values) move down the ladder of powers (that is, try square root, cube root, logarithmic, etc. transformations).\n",
    "\n",
    "If the data are left-skewed (clustered at higher values) move up the ladder of powers (cube, square, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://miro.medium.com/max/1400/1*RRZ4lakWAhBWRMC9r1r0Ew.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The boxcox() SciPy function implements the Box-Cox method. It takes an argument, called lambda, that controls the type of transform to perform.\n",
    "\n",
    "lambda = -1. is a reciprocal transform.\n",
    "lambda = -0.5 is a reciprocal square root transform.\n",
    "lambda = 0.0 is a log transform.\n",
    "lambda = 0.5 is a square root transform.\n",
    "lambda = 1.0 is no transform.\n",
    "\n",
    "A limitation of the Box-Cox transform is that it assumes that all values in the data sample are positive.\n",
    "\n",
    "Yeo-Johnson Transformation Method\n",
    "\n",
    "Unlike the Box-Cox transform, it does not require the values for each input variable to be strictly positive.\n",
    "It supports zero values and negative values. This means we can apply it to our dataset without scaling it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "outlier_handler = ColumnTransformer(transformers=[\n",
    "    ('outlier_remover', OutlierRemover(), cols)\n",
    "], \n",
    "                       remainder='passthrough')\n",
    "####\n",
    "####\n",
    "\n",
    "#\n",
    "class OutlierRemover(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,factor=1.5):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def outlier_detector(self,X,y=None):\n",
    "        X = pd.Series(X).copy()\n",
    "        q1 = X.quantile(0.25)\n",
    "        q3 = X.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        self.lower_bound.append(q1 - (self.factor * iqr))\n",
    "        self.upper_bound.append(q3 + (self.factor * iqr))\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.lower_bound = []\n",
    "        self.upper_bound = []\n",
    "        X.apply(self.outlier_detector)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for i in range(X.shape[1]):\n",
    "            x = X.iloc[:, i].copy()\n",
    "            x[(x < self.lower_bound[i]) | (x > self.upper_bound[i])] = np.nan\n",
    "            X.iloc[:, i] = x\n",
    "        return X\n",
    "    \n",
    "outlier_remover = OutlierRemover()\n",
    "\n",
    "test = pd.DataFrame({'col1':[100,200,300,999],'col2':[0,0,1,2],'col3':[-10,0,1,2]})\n",
    "test\n",
    "\n",
    "outlier_remover.fit_transform(test)\n",
    "\n",
    "#\n",
    "data.plot(kind=\"box\",subplots=True,figsize=(15,5),title=\"Data with Outliers\");\n",
    "\n",
    "#\n",
    "outlier_remover = OutlierRemover()\n",
    "\n",
    "#ColumnTransformer to remove outliers\n",
    "ct = ColumnTransformer(transformers=[['outlier_remover',OutlierRemover(),list(range(data.shape[1]))]],remainder='passthrough')\n",
    "\n",
    "#iris data after outlier removal\n",
    "data_without_outliers = pd.DataFrame(ct.fit_transform(data),columns=data.columns)\n",
    "\n",
    "#iris data box plot after outlier removal\n",
    "data_without_outliers.plot(kind=\"box\",subplots=True,figsize=(15,5),title=\"Data without Outliers\");\n",
    "\n",
    "# 4 outliers are removed from SepalWidthCm, other columns stayed the same as they have no outliers.\n",
    "data_without_outliers.isnull().sum()\n",
    "\n",
    "#outliers removed from sepal width (cm)\n",
    "list(data.loc[data_without_outliers.isnull().sum(axis=1)>0,'SepalWidthCm'])\n",
    "\n",
    "# Method 2\n",
    "def outlier_removal(X,factor):\n",
    "    X = pd.DataFrame(X).copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        x = pd.Series(X.iloc[:,i]).copy()\n",
    "        q1 = x.quantile(0.25)\n",
    "        q3 = x.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (factor * iqr)\n",
    "        upper_bound = q3 + (factor * iqr)\n",
    "        X.iloc[((X.iloc[:,i] < lower_bound) | (X.iloc[:,i] > upper_bound)),i] = np.nan \n",
    "    return X\n",
    "\n",
    "#creating outlier_remover object using FunctionTransformer with factor=1.5\n",
    "outlier_remover = FunctionTransformer(outlier_removal,kw_args={'factor':1.5})\n",
    "\n",
    "test = pd.DataFrame({'col1':[100,200,300,999],'col2':[0,0,1,2],'col3':[-10,0,1,2]})\n",
    "test\n",
    "\n",
    "outlier_remover.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_imputation_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "cat_imputation_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y Shape before LE: (891,) \n",
      "\n",
      "y Shape after LE as numpy: (891,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def label_encoding_handler(y):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "label_encoding_handler = LabelEncoder()\n",
    "####\n",
    "####\n",
    "\n",
    "print('y Shape before LE:', y_train.shape, '\\n')\n",
    "\n",
    "# Label encode target variable output as numpy array\n",
    "y_train_le = label_encoding_handler.fit_transform(y_train) \n",
    "print('y Shape after LE as numpy:', y_train.shape, '\\n')\n",
    "# print(y_train_le, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape before OHE: (891, 11) \n",
      "\n",
      "X Shape after OHE as CT numpy: (891, 13) \n",
      "\n",
      "[[1.0 0.0 1.0 ... 'A/5 21171' 7.25 nan]\n",
      " [0.0 0.0 0.0 ... 'PC 17599' 71.2833 'C85']\n",
      " [0.0 0.0 1.0 ... 'STON/O2. 3101282' 7.925 nan]\n",
      " ...\n",
      " [0.0 0.0 1.0 ... 'W./C. 6607' 23.45 nan]\n",
      " [1.0 0.0 0.0 ... '111369' 30.0 'C148']\n",
      " [1.0 1.0 0.0 ... '370376' 7.75 nan]] \n",
      "\n",
      "X Shape after OHE as pdf: (891, 13) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_nan</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex_male Embarked_Q Embarked_S Embarked_nan PassengerId Pclass  \\\n",
       "0      1.0        0.0        1.0          0.0           1      3   \n",
       "1      0.0        0.0        0.0          0.0           2      1   \n",
       "2      0.0        0.0        1.0          0.0           3      3   \n",
       "3      0.0        0.0        1.0          0.0           4      1   \n",
       "4      1.0        0.0        1.0          0.0           5      3   \n",
       "\n",
       "                                                Name   Age SibSp Parch  \\\n",
       "0                            Braund, Mr. Owen Harris  22.0     1     0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0     1     0   \n",
       "2                             Heikkinen, Miss. Laina  26.0     0     0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0     1     0   \n",
       "4                           Allen, Mr. William Henry  35.0     0     0   \n",
       "\n",
       "             Ticket     Fare Cabin  \n",
       "0         A/5 21171   7.2500   NaN  \n",
       "1          PC 17599  71.2833   C85  \n",
       "2  STON/O2. 3101282   7.9250   NaN  \n",
       "3            113803  53.1000  C123  \n",
       "4            373450   8.0500   NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "ohe_cols = ['Sex', 'Embarked']\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "\n",
    "one_hot_encoding_handler = ColumnTransformer(transformers=[\n",
    "    ('ohe', ohe, ohe_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "print('X Shape before OHE:', X_train.shape, '\\n')\n",
    "\n",
    "# Column transformer output as numpy array (default)\n",
    "X_train_ohe = one_hot_encoding_handler.fit_transform(X_train)\n",
    "print('X Shape after OHE as CT numpy:', X_train_ohe.shape, '\\n')\n",
    "print(X_train_ohe, '\\n')\n",
    "\n",
    "# Dataframe output\n",
    "X_train_ohe = pd.DataFrame(X_train_ohe, columns=one_hot_encoding_handler.get_feature_names_out())\n",
    "print('X Shape after OHE as pdf:', X_train_ohe.shape, '\\n')\n",
    "X_train_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape before Ordinal Encoding: (891, 13) \n",
      "\n",
      "X Shape after Ordinal Encoding as CT numpy: (891, 13) \n",
      "\n",
      "[[2.0 1.0 0.0 ... 'A/5 21171' 7.25 nan]\n",
      " [0.0 0.0 0.0 ... 'PC 17599' 71.2833 'C85']\n",
      " [2.0 0.0 0.0 ... 'STON/O2. 3101282' 7.925 nan]\n",
      " ...\n",
      " [2.0 0.0 0.0 ... 'W./C. 6607' 23.45 nan]\n",
      " [0.0 1.0 0.0 ... '111369' 30.0 'C148']\n",
      " [2.0 1.0 1.0 ... '370376' 7.75 nan]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_ordinal</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_nan</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Pclass_ordinal Sex_male Embarked_Q Embarked_S Embarked_nan PassengerId  \\\n",
       "0            2.0      1.0        0.0        1.0          0.0           1   \n",
       "1            0.0      0.0        0.0        0.0          0.0           2   \n",
       "2            2.0      0.0        0.0        1.0          0.0           3   \n",
       "3            0.0      0.0        0.0        1.0          0.0           4   \n",
       "4            2.0      1.0        0.0        1.0          0.0           5   \n",
       "\n",
       "                                                Name   Age SibSp Parch  \\\n",
       "0                            Braund, Mr. Owen Harris  22.0     1     0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0     1     0   \n",
       "2                             Heikkinen, Miss. Laina  26.0     0     0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0     1     0   \n",
       "4                           Allen, Mr. William Henry  35.0     0     0   \n",
       "\n",
       "             Ticket     Fare Cabin  \n",
       "0         A/5 21171   7.2500   NaN  \n",
       "1          PC 17599  71.2833   C85  \n",
       "2  STON/O2. 3101282   7.9250   NaN  \n",
       "3            113803  53.1000  C123  \n",
       "4            373450   8.0500   NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ordinal_encoding_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "ordinal_cols = ['Pclass']\n",
    "\n",
    "ordinal_feat_1_categories = [1, 2, 3]\n",
    "# ordinal_feat_2_categories = ['first', 'second', 'third']\n",
    "\n",
    "ordinal = OrdinalEncoder(categories=[ordinal_feat_1_categories])\n",
    "\n",
    "ordinal_encoding_handler = ColumnTransformer(transformers=[\n",
    "    ('ordinal', ordinal, ordinal_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "####\n",
    "####\n",
    "\n",
    "print('X Shape before Ordinal Encoding:', X_train_ohe.shape, '\\n')\n",
    "\n",
    "# Column transformer output as numpy array (default)\n",
    "X_train_ordinal = ordinal_encoding_handler.fit_transform(X_train_ohe)\n",
    "print('X Shape after Ordinal Encoding as CT numpy:', X_train_ordinal.shape, '\\n')\n",
    "print(X_train_ordinal, '\\n')\n",
    "\n",
    "# Dataframe output\n",
    "X_train_ordinal = pd.DataFrame(X_train_ordinal, columns=ordinal_cols + list(X_train_ohe.drop(columns=ordinal_cols).columns))\n",
    "X_train_ordinal.columns = [col +'_ordinal' if col in ordinal_cols else col for col in X_train_ordinal.columns]\n",
    "X_train_ordinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "target_encoding_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text handlers\n",
    "def vectorizer_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "vectorizer_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model input transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input handlers\n",
    "def scaling_handler(X):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "scaling_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbalance_handler(df):\n",
    "    pass\n",
    "\n",
    "####\n",
    "####\n",
    "imbalance_handler =\n",
    "####\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "###################\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# \n",
    "from sklearn import set_config                      # to change the display\n",
    "from sklearn.utils import estimator_html_repr       # to save the diagram into HTML format\n",
    "\n",
    "# Validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Evaluation / Scoring metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "# X, y = load_breast_cancer(return_X_y = True, as_frame=True)\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "X.drop('CHAS', axis=1, inplace=True)\n",
    "y = pd.Series(boston.target, name='MEDV')\n",
    "\n",
    "# inspect data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "#                                                     stratify=y,\n",
    "                                                    random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes\n",
    "X_train.shape\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = X_train.select_dtypes(include=['object']).columns\n",
    "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# cat_features = []\n",
    "# num_features = []\n",
    "\n",
    "# print(cat_features)\n",
    "# print(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_features)\n",
    "len(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing.outliers import boxplot_numeric_features, IQR_Outliers, CustomSampler_IQR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Numeric Outlier (IQR)\n",
    "Z-Score\n",
    "DBSCAN\n",
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "rows_for_plot = 6\n",
    "cols_for_plot = 5\n",
    "###\n",
    "\n",
    "boxplot_numeric_features(X_train,\n",
    "                         rows_for_plot=rows_for_plot,\n",
    "                         cols_for_plot=cols_for_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before IQR outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after IQR outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "X = X_train\n",
    "z_score_thresh = 3\n",
    "\n",
    "print(\"Shape before Z-score outlier removal:\", X.shape)\n",
    "\n",
    "print(\"Shape after Z-score outlier removal:\", X_o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR_Outliers(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomSampler_IQR(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=11)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on training dataset with outliers removed\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# # select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask], y_train[mask]\n",
    "\n",
    "# # summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "### Model for coefficients of features\n",
    "model = LogisticRegression(random_state=11)\n",
    "s_scaler = StandardScaler()\n",
    "###\n",
    "\n",
    "# Pipeline with Scaler\n",
    "pipeline_scaler = Pipeline([\n",
    "    ('scaler', s_scaler),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline_scaler.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline_scaler.predict(X_train)\n",
    "print(\"Accuracy on Train set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline_scaler.predict(X_test)\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")\n",
    "\n",
    "# # evaluate the model\n",
    "# yhat = model.predict(X_test)\n",
    "\n",
    "# # evaluate predictions\n",
    "# mae = mean_absolute_error(y_test, yhat)\n",
    "# print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "# Do i want to remove outliers from test dataset? Without the outlier tows then no prediction can be made\n",
    "LR_Pipeline = Pipeline([\n",
    "    ('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False)),\n",
    "    ('Imputer', SimpleImputer(strategy = \"median\")),\n",
    "    ('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat_transformer', cat_pipe, cat_features),\n",
    "    ('num_transformer', num_pipe, num_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict training data\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "# print(f\"Predictions on training data: {y_train_pred}\")\n",
    "print(\"Accuracy on Training set:\", accuracy_score(y_train, y_train_pred), \"\\n\")\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "# print(f\"Predictions on test data: {y_test_pred}\")\n",
    "print(\"Accuracy on Test set:\", accuracy_score(y_test, y_test_pred), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set config to diagram for visualizing the pipelines/composite estimators\n",
    "set_config(display='diagram')\n",
    "\n",
    "# Lets visualize the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Num_vars is the list of numerical variables \n",
    "X_train_num = X_train[numeric_features]\n",
    "X_train_num = imputer.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "X_train_cat = X_train[categorical_features]\n",
    "X_train_cat_ord_encoded = ordinal_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_ord_encoded[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()\n",
    "\n",
    "X_train_cat_hot_encoded = cat_encoder.fit_transform(X_train_cat)\n",
    "X_train_cat_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome transformations\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "ratings_index = -2\n",
    "reviews_index = -1\n",
    "class NewVariablesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "    # Make a new variable that is rating divided by number of reviews\n",
    "        ratings_over_reviews = X[:,ratings_index]/X[:,reviews_index]\n",
    "        return np.c_[X, ratings_over_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('add_variables', NewVariablesAdder()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_num_transformed = num_pipeline.fit_transform(X_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, num_vars),\n",
    "    ('categorical', OneHotEncoder(), cat_vars),\n",
    "    \n",
    "])\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('features', FeatureUnion ([\n",
    "     ('Cat Columns', Pipeline([\n",
    "          ('Category Extractor', TypeSelector(np.number)),\n",
    "                 ('Impute Zero', SimpleImputer(strategy=\"constant\", fill_value=0))\n",
    "                                    ])),\n",
    "('Numerics', Pipeline([\n",
    "      ('Numeric Extractor', TypeSelector(\"category\")),\n",
    "          ('Impute Missing', SimpleImputer(strategy=\"constant\", fill_value='missing'))\n",
    "          ]))        \n",
    "     ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'median', fill_value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age', 'fare']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['embarked', 'sex', 'pclass']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OrdinalEncoder())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate count statistics of duplicate entries\n",
    "print (\"## Number of duplicate rows ## \\n\")\n",
    "if len(X_y_data[X_y_data.duplicated()]) > 0:\n",
    "    print(\"Number of duplicated observations: \", len(X_y_data[X_y_data.duplicated()]))\n",
    "    X_y_data[X_y_data.duplicated(keep=False)].sort_values(by=list(X_y_data.columns)).head()\n",
    "else:\n",
    "    print(\"No duplicated observations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_y_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with a lot of missing values.\n",
    "ind_missing = df[df['num_missing'] > 35].index\n",
    "df_less_missing_rows = df.drop(ind_missing, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_beds_raion has a lot of missing.\n",
    "# If we want to drop.\n",
    "cols_to_drop = ['hospital_beds_raion']\n",
    "df_less_hos_beds_raion = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with the median.\n",
    "med = df['life_sq'].median()\n",
    "print(med)\n",
    "df['life_sq'] = df['life_sq'].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each numeric column.\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "numeric_cols = df_numeric.columns.values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        med = df[col].median()\n",
    "        df[col] = df[col].fillna(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute the missing values and create the missing value indicator variables for each non-numeric column.\n",
    "df_non_numeric = df.select_dtypes(exclude=[np.number])\n",
    "non_numeric_cols = df_non_numeric.columns.values\n",
    "\n",
    "for col in non_numeric_cols:\n",
    "    missing = df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    \n",
    "    if num_missing > 0:  # only do the imputation for the columns that have missing values.\n",
    "        print('imputing missing values for: {}'.format(col))\n",
    "        df['{}_ismissing'.format(col)] = missing\n",
    "        \n",
    "        top = df[col].describe()['top'] # impute with the most frequent value.\n",
    "        df[col] = df[col].fillna(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical\n",
    "df['sub_area'] = df['sub_area'].fillna('_MISSING_')\n",
    "\n",
    "\n",
    "# numeric\n",
    "df['life_sq'] = df['life_sq'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This article covers 7 ways to handle missing values in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deleting Rows with missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for continuous variable\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].mean())\n",
    "data[\"Age\"] = data[\"Age\"].replace(np.NaN, data[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Impute missing values for categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other Imputation Methods\n",
    "\n",
    "data[\"Age\"] = data[\"Age\"].fillna(method='ffill')\n",
    "data[\"Age\"] = data[\"Age\"].interpolate(method='linear', limit_direction='forward', axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Algorithms that support missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction of missing values\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data = data[[\"Survived\", \"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]]\n",
    "\n",
    "data[\"Sex\"] = [1 if x==\"male\" else 0 for x in data[\"Sex\"]]\n",
    "\n",
    "test_data = data[data[\"Age\"].isnull()]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "y_train = data[\"Age\"]\n",
    "X_train = data.drop(\"Age\", axis=1)\n",
    "X_test = test_data.drop(\"Age\", axis=1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imputation using Deep Learning Library — Datawig\n",
    "\n",
    "import pandas as pd\n",
    "#pip install datawig\n",
    "import datawig\n",
    "\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(data)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['Pclass','SibSp','Parch'], # column(s) containing information about the column we want to impute\n",
    "    output_column= 'Age', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df_train, num_epochs=50)\n",
    "\n",
    "#Impute missing values and return original dataframe with predictions\n",
    "imputed = imputer.predict(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do All Data Cleaning on Only Train Set and Apply Calculations to Validation/Test Later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_ml_data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "Image(url= \"https://miro.medium.com/max/700/1*_RA3mCS30Pr0vUxbp25Yxw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Redundant/Irrelevant Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_redundant_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_class_imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Category Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_category_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Rescaling: Standardise/Normalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ml_rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Other Distribution Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TO LOOK INTO NEXT PREPROCESSING PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET ENCODER LOOK INTO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS FOR ALL PREPROCESSING STEPS TO FEED INTO PIPELINES\n",
    "\n",
    "imputers\n",
    "scaler\n",
    "pca\n",
    "smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', categorical_transformer, categorical_features)\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor',RandomForestRegressor())\n",
    "           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
